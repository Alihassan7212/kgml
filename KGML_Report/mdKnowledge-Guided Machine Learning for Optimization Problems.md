# **Knowledge-Guided Machine Learning for Optimization Problems: A Comprehensive Analysis**

## **1\. Introduction to Knowledge-Guided Machine Learning for Optimization**

### **1.1. The Imperative for Knowledge Integration in ML for Optimization**

The application of machine learning (ML) to solve complex optimization problems has garnered significant attention. However, purely data-driven ML models often encounter substantial limitations. These models are frequently criticized for being "black boxes" with limited interpretability, making it difficult to understand their decision-making processes.1 This opacity can hinder trust and adoption, especially in critical scientific and engineering applications. Furthermore, such models may exhibit poor generalization to out-of-distribution samples and can produce solutions that are physically implausible or violate known domain constraints.2 Optimization problems, particularly those rooted in scientific and engineering disciplines, are often governed by well-established principles, physical laws, mathematical properties, or expert-derived heuristics. Disregarding this wealth of existing knowledge is not only inefficient but can also lead to ML solutions that are suboptimal, unreliable, or scientifically unsound.

The drive towards integrating domain knowledge into ML frameworks, often termed Knowledge-Guided Machine Learning (KGML), represents a fundamental shift. This evolution is not merely about enhancing the general performance of ML models but is crucial for transforming ML into a more rigorous, trustworthy, and effective tool for scientific discovery and engineering design. This is particularly pertinent in the field of optimization, where solutions must be not only statistically optimal but also valid and interpretable within their specific domain context. Traditional ML models, while powerful pattern recognizers, often falter in scientific domains precisely because of their "black-box" nature and their inherent inability to inherently respect known physical laws or operational constraints.1 Given that optimization problems in science and engineering are deeply intertwined with established domain knowledge (e.g., laws of physics, chemical reactions, operational rules), applying ML without this explicit knowledge is akin to attempting to re-discover fundamental principles from data alone. Such an approach is inefficient and highly susceptible to errors or the generation of physically implausible solutions. KGML emerges as a direct response to these inherent limitations, seeking to create a robust synergy between the pattern-recognition strengths of data-driven learning and the established principles of knowledge-based reasoning.2 Consequently, KGML is not an incremental improvement but a necessary evolutionary step for ML to achieve true efficacy and reliability in tackling complex, knowledge-rich optimization tasks, ensuring that derived solutions are not just statistically indicated but are also scientifically sound, verifiable, and explainable. The integration of domain knowledge is posited to lead to ML models for optimization that are more robust, scientifically consistent, explainable, and data-efficient, particularly in scenarios where high-quality labeled data is scarce.1

### **1.2. Defining Knowledge-Guided Machine Learning (KGML)**

Knowledge-Guided Machine Learning (KGML) is an emerging and rapidly advancing field of research that focuses on the deep and meaningful integration of scientific and domain-specific knowledge into the frameworks of machine learning.1 Unlike purely data-driven approaches that learn exclusively from observed data, KGML leverages existing knowledge—such as physical laws, mathematical principles, simulation outputs, or expert heuristics—as a complementary source of information to guide the design, training, and evaluation of ML models.2 The primary aim of KGML is to develop models that exhibit superior generalizability, particularly to unseen or out-of-distribution data; ensure scientific consistency, meaning their predictions adhere to known principles; enhance the explainability of their results, moving away from the "black-box" paradigm; and achieve improved performance, especially in situations characterized by limited or sparse data.1

KGML represents a distinct departure from conventional data-only methods by explicitly incorporating established knowledge. This integration can occur at various stages of the ML pipeline, including data preprocessing, model architecture design, loss function formulation, or the learning process itself.7 By doing so, KGML seeks to constrain the hypothesis space of ML models, guiding them towards solutions that are not only statistically sound but also align with the underlying principles of the domain being modeled.

The role of KGML extends beyond merely using existing knowledge to enhance machine learning models; it also establishes a framework through which this knowledge can be tested, refined, and, in some instances, even lead to the discovery of new domain insights. This dual functionality arises from the explicit incorporation of domain knowledge—be it physical laws, operational constraints, or expert heuristics—into the ML architecture or learning process.1 When a KGML model, which has been infused with specific knowledge, is evaluated, its performance relative to purely data-driven or purely knowledge-driven models can offer significant insights into the utility and accuracy of the incorporated knowledge itself.8 For instance, if a KGML model that includes particular knowledge constraints underperforms, it may signal that the knowledge component is incomplete, inaccurately applied to the specific context, or that its interaction with the available data is not optimally modeled. Conversely, superior performance by a KGML model can serve as a validation of the embedded knowledge and its chosen representation. Moreover, a detailed analysis of the learned components within a KGML model—such as how data-driven elements adapt around the imposed knowledge constraints—can lead to the formulation of new hypotheses or refinements to existing domain understanding. This iterative process of building, evaluating, and analyzing KGML models is particularly valuable in scientific discovery, where the goal is often not just prediction but also deeper comprehension of underlying mechanisms.2

### **1.3. Overview of Optimization Problems and Their Challenges**

Optimization problems are ubiquitous, forming the core of decision-making processes in a vast array of fields, including science, engineering, economics, finance, and industrial operations.10 Fundamentally, an optimization problem involves finding the "best" solution from a set of available or feasible solutions, where "best" is defined with respect to one or more objective functions, often subject to a series of constraints that the solution must satisfy.12 These problems can range from determining the most efficient route for a delivery network to designing a novel material with specific desired properties or allocating scarce resources to maximize a particular outcome.

Despite their prevalence, solving optimization problems presents numerous significant challenges:

1. **High Dimensionality and Large Search Spaces:** Many real-world optimization problems involve a large number of variables, leading to exponentially vast search spaces that are computationally prohibitive to explore exhaustively.  
2. **Computational Complexity:** A significant class of optimization problems, particularly in combinatorial optimization, are NP-hard. This implies that the time required to find an exact optimal solution grows exponentially with the problem size, rendering exact methods impractical for large instances.13  
3. **Multiple Conflicting Objectives:** Often, decision-makers need to optimize several objectives simultaneously, which may be conflicting (e.g., minimizing cost while maximizing quality). This necessitates finding a set of trade-off solutions (Pareto optimal solutions) rather than a single optimum.  
4. **Nature of Objective Functions:** Objective functions can be noisy, making it difficult to ascertain true improvements. They might also be expensive to evaluate, limiting the number of possible solution candidates that can be assessed. In many cases, the objective function is a "black box," meaning its analytical form is unknown, and it can only be queried for values at specific points.10  
5. **Model Formulation:** Accurately formulating a mathematical model that captures all the intricacies and constraints of a real-world scenario can be extremely challenging. Human-expert derived formulations might not fully encapsulate the complexities, potentially leading to suboptimal solutions when the model is optimized.14

The "No Free Lunch" (NFL) theorem in optimization further underscores these challenges by stating that no single optimization algorithm can be expected to perform best on all possible classes of optimization problems.14 This implies the need for tailored approaches that can effectively exploit the specific structures and characteristics of the problem at hand.

The inherent difficulties encountered in traditional optimization methods—such as computational complexity, the handling of black-box or expensive objective functions, and the intricacies of multi-objective decision-making—are not merely issues that KGML *can* address, but rather they serve as primary *catalysts* for its development and the innovation of specific KGML methodologies. For instance, the NP-hard nature of many combinatorial optimization problems 13 necessitates the use of heuristics. KGML offers a pathway to learn these heuristics or to improve existing ones by systematically incorporating problem-specific knowledge.16 When objective functions are expensive to evaluate or are effectively black boxes 10, KGML techniques such as Bayesian Optimization come to the fore. These methods construct surrogate models of the objective function, guided by prior beliefs (a form of knowledge), to navigate the solution space with greater data efficiency.10 Furthermore, real-world problems frequently involve complex constraints or poorly understood system dynamics that are difficult to capture in precise mathematical formulations.14 KGML allows for learning from available data while being guided by partial or qualitative knowledge, such as enforcing physical laws as soft constraints within the learning process.17 In the realm of multi-objective optimization, where solutions involve intricate trade-offs 18, KGML can facilitate a more effective exploration of the Pareto front by incorporating knowledge about desirable solution characteristics or known relationships between objectives.18 Thus, the specific challenges endemic to optimization directly spur the creation of KGML approaches designed to surmount these obstacles, positioning KGML as a problem-driven field aimed at enhancing our ability to find robust and meaningful solutions.

### **1.4. The Deakin-Coventry Cotutelle Program: A Focused Initiative**

Illustrating the academic and research commitment to this burgeoning field is the Deakin-Coventry cotutelle PhD scholarship program, specifically focused on "Knowledge-Guided Machine Learning for Optimization Problems".20 This collaborative initiative underscores the timeliness and relevance of KGML in addressing notoriously complex optimization challenges. The program's stated aim is "to develop Machine Learning models to develop solutions to optimization algorithms that have notoriously high complexity" 21, directly targeting the core issues discussed previously.

The scholarship description explicitly identifies several classic optimization problems as areas of research focus, including the Travelling Salesperson Problem (TSP), Convex Hull, and Delaunay Triangulation.20 These problems are well-known for their combinatorial complexity and serve as important benchmarks for novel optimization techniques. Furthermore, the program highlights specific advanced ML models to be explored, such as Pointer Networks and transformer-based models.20 The selection of these architectures indicates a research direction towards leveraging sophisticated deep learning techniques, guided by KGML principles, to tackle these difficult problems. This focused initiative serves as a concrete example of ongoing efforts to push the frontiers of KGML in the optimization domain, framing the detailed discussions within this report in the context of active and pertinent research endeavors.

The specific research direction of the Deakin-Coventry scholarship—targeting computationally intensive combinatorial problems like the TSP using advanced neural architectures such as Transformers, all under the guiding principles of KGML—is indicative of a significant and broader trend within the KGML for optimization landscape. This trend involves confronting notoriously difficult optimization problems with sophisticated, knowledge-infused deep learning methodologies. The scholarship's explicit mention of "Knowledge-guided Machine Learning" as a growing trend, coupled with its objective to apply ML to optimization problems characterized by "notoriously high complexity" 21, sets the stage. The chosen problems—TSP, Convex Hull, and Delaunay Triangulation—are archetypal examples of combinatorial optimization, many of which are NP-hard and demand highly refined solution strategies.13 The proposed ML models, Pointer Networks and Transformers 21, represent cutting-edge deep learning architectures. Pointer Networks, for instance, are specifically designed for problems where output elements are pointers to input elements, a structure that aligns naturally with the task of ordering cities in the TSP. Transformers, with their powerful attention mechanisms, offer robust capabilities for modeling complex sequences and dependencies. This strategic alignment of complex problems with advanced, knowledge-aware ML models mirrors a wider ambition in the field: to develop end-to-end or hybrid ML solutions capable of learning effective heuristics or solution construction policies for intractable optimization tasks.16 Thus, the Deakin-Coventry scholarship serves as a specific instantiation of the broader aspiration within KGML to expand the boundaries of what is achievable in optimization through artificial intelligence.

## **2\. Foundations of Knowledge-Guided Machine Learning**

### **2.1. Core Principles and Paradigms of KGML**

The integration of domain knowledge into machine learning models is the cornerstone of KGML. This integration is not monolithic but is achieved through several distinct, yet often complementary, paradigms. Literature consistently identifies three main approaches through which KGML operates, each targeting a different aspect of the ML pipeline to infuse knowledge and guide the learning process towards more robust, interpretable, and scientifically sound solutions.1

#### **2.1.1. Knowledge-Guided Learning (Modifying Loss Functions)**

This paradigm focuses on incorporating scientific or domain knowledge directly into the learning objective of the ML model. This is typically achieved by modifying the loss function, which the ML algorithm seeks to minimize during training. The standard data-driven loss (e.g., mean squared error for regression, cross-entropy for classification) is augmented with additional terms that penalize the model if its predictions or internal states violate known physical laws, conservation principles, mathematical properties, or other forms of established knowledge.1 For example, in a system where energy conservation is a known principle, a penalty term could be added to the loss if the model's predictions imply a creation or destruction of energy.17 By including such physics-informed regularizers, the optimization process that trains the ML model is guided not only towards fitting the observed data but also towards solutions that are consistent with the underlying science of the domain. This approach aims to produce models that are more generalizable, as they learn features that are scientifically plausible, and more reliable, especially when extrapolating beyond the training data distribution.1

#### **2.1.2. Knowledge-Guided Architecture (Embedding Knowledge in Model Structure)**

The second major paradigm involves embedding scientific knowledge directly into the architecture or structure of the ML model itself.1 Instead of, or in addition to, modifying the learning objective, this approach designs the model's components—such as layers in a neural network, connections between nodes, or the overall flow of computation—to inherently reflect known physical properties, conservation laws, causal relationships, or structural constraints of the problem domain. For instance, a neural network layer might be specifically designed to model a known physical interaction, or the connectivity in a Graph Neural Network (GNN) might represent known physical or logical relationships between entities.1 In some cases, ML models can be decomposed into sub-modules, where each sub-module corresponds to a distinct process or component whose interactions are guided by domain knowledge.23 By encoding knowledge into the architecture, the model is inherently biased to learn representations and functions that are consistent with this knowledge, which can lead to improved interpretability, robustness, and data efficiency, as the model does not need to learn these fundamental principles from scratch solely from data.

#### **2.1.3. Knowledge-Guided Pretraining (Using Simulated or Principled Data)**

Knowledge-guided pretraining leverages data that already encodes scientific principles, often generated from physics-based simulations or through self-supervised learning tasks designed around known invariances or laws.1 The ML model is first pretrained on this large corpus of knowledge-rich data. This pretraining phase allows the model to learn initial parameter values that reflect the underlying scientific understanding of the domain. Subsequently, the pretrained model can be fine-tuned on smaller, potentially sparse, real-world observational datasets. This approach is particularly beneficial in data-scarce scenarios, where directly training a complex ML model on limited observational data might lead to overfitting or poor generalization. The knowledge embedded in the simulated or principled pretraining data provides a strong inductive bias, helping the model to learn more meaningful and generalizable features, thereby reducing the reliance on large volumes of labeled observational data and improving performance when such data is limited.1 For example, a model might be pretrained using extensive data from a detailed climate simulation before being fine-tuned with sparse real-world weather station readings.1

These three core paradigms—Knowledge-Guided Learning, Knowledge-Guided Architecture, and Knowledge-Guided Pretraining—are not mutually exclusive. In practice, they are often most potent when strategically combined, leading to hybrid KGML approaches that are tailored to the specific complexities of the problem at hand and the nature of the available knowledge and data. Each paradigm offers a distinct entry point for knowledge integration: influencing the learning objective (Learning), shaping the model's internal structure (Architecture), or initializing the model's understanding through data (Pretraining).1 Complex optimization problems, with their multifaceted constraints and dynamics, frequently benefit from such multi-pronged knowledge infusion. For instance, a sophisticated KGML system might employ knowledge-guided pretraining using data generated by a high-fidelity physics-based simulator.1 This could be followed by designing a knowledge-guided architecture that inherently respects certain physical symmetries or constraints relevant to the optimization problem.2 Finally, during the fine-tuning phase on real-world data, a knowledge-guided loss function could be employed to enforce critical conservation laws or operational rules.1 The KGML-SM framework for corn yield prediction, for example, implicitly uses knowledge in its architecture by simulating intermediate variables representing key plant growth processes and potentially in its learning objective through a drought-aware loss function.1 Similarly, Physics-Informed Neural Networks (PINNs) 3, a prominent KGML technique, primarily utilize knowledge-guided learning by incorporating physics-based regularization terms into their loss functions, but their effective implementation can also involve careful architectural choices. Therefore, the practical and impactful application of KGML often necessitates a thoughtful combination of these paradigms, rather than a rigid adherence to a single approach, to best leverage the complementary strengths of available domain knowledge and empirical data.

The following table provides a comparative overview of these KGML paradigms:

Table 1: Comparison of KGML Paradigms

| Feature | Knowledge-Guided Learning | Knowledge-Guided Architecture | Knowledge-Guided Pretraining |
| :---- | :---- | :---- | :---- |
| **Mechanism** | Modifying the loss function to include knowledge-based penalties or rewards. | Embedding knowledge directly into the model's structure, layers, or connectivity. | Using data generated from known principles (e.g., simulations) to initialize model parameters. |
| **Primary Goal** | Ensure solutions are scientifically/physically consistent; guide learning towards valid regions. | Build models that inherently respect known relationships or constraints; improve interpretability. | Initialize model with scientific understanding; improve performance in data-scarce scenarios. |
| **Examples** | Physics-Informed Neural Networks (PINNs) 17; adding conservation law residuals to loss.1 | Graph Neural Networks with physics-informed graph structures 23; custom layers for physical processes.1 | Pretraining on climate model outputs 1; using KGPFA with simulated crop data.24 |
| **Strengths** | Flexible integration of diverse constraints; direct impact on optimization objective. | Strong inductive bias; can lead to more interpretable models; inherently enforces some knowledge. | Reduces need for large observational datasets; good starting point for fine-tuning; leverages existing simulators. |
| **Limitations/Challenges** | Defining appropriate penalty terms; balancing data-fit vs. knowledge-fit; potential for complex loss landscapes. | Can be difficult to design for complex knowledge; may restrict model flexibility; architectural changes can be intricate. | Simulation data may not perfectly match reality (sim-to-real gap); pretraining can be computationally expensive. |
| **Typical Use Cases in Optimization** | Enforcing physical constraints in continuous optimization (e.g., fluid dynamics, structural optimization); ensuring feasible solutions in constrained optimization. | Designing solvers for structured problems (e.g., Pointer Networks for TSP 21); modeling systems with known component interactions. | Learning heuristics for problems where simulators exist; parameter estimation in systems with well-understood physics but sparse real data. |

### **2.2. Types of Domain Knowledge for Integration**

The efficacy of KGML hinges on the type and quality of domain knowledge available for integration. This knowledge can vary significantly in its nature, formality, and source. Broadly, it can be categorized into scientific knowledge, world knowledge, and expert knowledge, each presenting unique opportunities and challenges for integration into ML models.7

#### **2.2.1. Scientific Knowledge**

Scientific knowledge comprises formally established and rigorously validated information derived from the natural sciences (physics, chemistry, biology), engineering disciplines, mathematics, and computer science. This category includes fundamental physical laws (e.g., Newton's laws of motion, laws of thermodynamics), governing differential equations (e.g., Navier-Stokes equations in fluid dynamics, Maxwell's equations in electromagnetism), conservation principles (e.g., conservation of mass, energy, momentum), and established mathematical theorems or properties.7 Due to its formal, often mathematical, representation, scientific knowledge is generally the most straightforward to integrate into ML models. It can be incorporated as hard constraints that solutions must satisfy, as soft constraints via penalty terms in loss functions (common in PINNs), or by embedding these equations or principles directly into the architecture of the ML model.7

#### **2.2.2. World Knowledge**

World knowledge, often referred to as common-sense knowledge, encompasses the vast body of intuitive, everyday information that humans acquire through perception, experience, and interaction with the environment. This type of knowledge is typically not formally codified or scientifically validated in the same way as scientific knowledge but is crucial for understanding context and making reasonable inferences. Examples include semantic relationships between concepts (e.g., "a cat is a mammal"), intuitive physics (e.g., an unsupported object will fall), general facts about the environment (e.g., water is wet), and social conventions.7 Integrating world knowledge into ML models is more challenging due to its often implicit and less structured nature. However, it can provide valuable priors for ML models, particularly in fields like natural language processing (where understanding semantics is key), computer vision (for scene understanding), and cognitive AI systems that aim to mimic human-like reasoning. Large Language Models (LLMs), for instance, implicitly capture a vast amount of world knowledge through pretraining on massive text corpora.

#### **2.2.3. Expert Knowledge**

Expert knowledge is specialized, domain-specific information, insights, and heuristics held by professionals or practitioners within a particular field. This knowledge is accumulated through extensive training, experience, and practice. It may include diagnostic rules used by medical doctors, engineering design heuristics employed by seasoned engineers, financial forecasting models based on expert analysts' insights, or best practices in a specific operational domain.7 While expert knowledge is grounded in theoretical or practical fundaments, it may not always be as rigorously formalized as scientific knowledge and can sometimes be tacit or subjective. Integration of expert knowledge into ML can occur through various means, such as: guiding the feature engineering process, developing rule-based systems that work in conjunction with ML components, defining constraints based on expert rules-of-thumb, or using expert feedback to guide the learning process (e.g., in reinforcement learning). Expert knowledge is particularly valuable in domains where empirical data is scarce, expensive to obtain, or highly specialized, such as in medicine, finance, or complex engineering design.7

The ease with which these diverse types of domain knowledge can be integrated into KGML systems, and the specific methods chosen for this integration, are profoundly influenced by the knowledge's degree of formalization. A clear spectrum exists, from highly formalized scientific principles to less structured expert heuristics and intuitive world knowledge. Scientific knowledge, often expressed in the precise language of mathematics through equations and established laws 7, lends itself most directly to integration. Physical laws, such as partial differential equations, can be translated into terms within a loss function, as seen in Physics-Informed Neural Networks 17, or can inspire specific architectural components in a neural network. This high level of formalization allows for a more direct and rigorous embedding of constraints.

Expert knowledge, which encompasses heuristics, domain-specific rules, and practical insights 7, occupies a middle ground. While sometimes formalized (e.g., in standard operating procedures or design codes), it often includes tacit elements. Integration might involve translating these heuristics into rule-based systems that interface with ML models, using them to guide the feature engineering process 8, or employing them to define the structure or parameters of an optimization algorithm. The recent use of LLMs to propose or refine heuristics based on an "expert-like" understanding of a problem domain also falls into this category.22

World knowledge, characterized by its intuitive and common-sense nature 7, presents the greatest challenge for direct formalization and integration. It is typically not expressed in mathematical terms. Its incorporation into ML systems often occurs more implicitly, for example, through the vast amounts of text and code data used to pretrain LLMs, which allows these models to capture some aspects of common-sense reasoning. Alternatively, architectural designs that aim to mimic cognitive processes might be seen as an indirect way of embedding world knowledge. As noted, scientific knowledge is "generally the easiest to encorporate, as it is well-formalized," whereas world knowledge "lacks formal scientific validation," and expert knowledge "may not always be mathematically formalized".7 Consequently, the toolkit for KGML must be diverse and adaptable, offering a range of methods suited to this entire spectrum of knowledge formality—from the direct embedding of mathematical equations to the use of more abstract architectural biases or heuristic guidance mechanisms.

### **2.3. Methods of Knowledge Infusion into ML Models**

Building upon the core paradigms of KGML and the types of knowledge available, several specific techniques are employed to infuse this knowledge into ML models. These methods operationalize the integration process, allowing abstract knowledge to shape the learning and behavior of the ML system.

#### **2.3.1. Feature Engineering and Domain-Aware Labeling**

One of the most fundamental ways to incorporate domain knowledge is through feature engineering and domain-aware labeling.7 Feature engineering involves using domain expertise to select, transform, or create input variables (features) that are more informative and relevant to the learning task. For instance, in predicting the properties of a new material, knowledge of physics and chemistry can guide the selection of features representing the atomic structure, composition, and bonding characteristics of its constituent elements.18 Domain-aware labeling involves using expert knowledge to refine or create target labels for supervised learning tasks, ensuring they accurately reflect the underlying phenomena or desired outcomes. This initial shaping of the data itself provides a strong knowledge-based foundation for the ML model.

#### **2.3.2. Model Constraints (Monotonicity, Linearity, Sparsity, Smoothness, Cyclicity, Range)**

Explicit constraints can be imposed on the ML model's behavior or the relationships it learns, ensuring alignment with known properties or expectations from the domain.8 These constraints can take various forms:

* **Monotonicity:** Ensuring that the model's output consistently increases or decreases with an increase in a specific input feature (e.g., a credit risk score should not increase with higher income).  
* **Linearity:** Restricting the model to learn a linear relationship between certain inputs and the output.  
* **Sparsity:** Limiting the number of input features the model uses for prediction, which can be useful for interpretability or when domain knowledge suggests only a few factors are critical.  
* **Smoothness:** Requiring the model's output to change smoothly in response to small changes in input features, relevant when abrupt changes are not physically expected.  
* **Cyclicity:** A form of smoothness constraint for features with cyclical patterns (e.g., time of day, day of the year), ensuring that predictions at the end of one cycle transition smoothly to the beginning of the next.  
* **Range Constraints:** Restricting the model's predictions to a predefined, physically plausible range (e.g., pH values must be between 0 and 14). These constraints can be implemented through model-agnostic preprocessing techniques or by selecting ML models that inherently support such constraints (e.g., XGBoost for monotonicity) or by designing custom neural network layers.8

#### **2.3.3. Physics-Informed Loss Functions and Regularization**

This method, which aligns closely with the "Knowledge-Guided Learning" paradigm, involves modifying the model's loss function to include terms that penalize deviations from known physical laws, conservation principles, or other scientific constraints.3 For example, in Physics-Informed Neural Networks (PINNs), the loss function typically includes a data fidelity term (how well the model fits observed data) and one or more physics-based regularization terms that measure the extent to which the neural network's output satisfies a given set of partial differential equations (PDEs) and boundary/initial conditions.8 This forces the model to learn solutions that are not only data-consistent but also physically consistent. Structured regularization, where the regularization term itself is designed based on domain knowledge, also falls under this category.

#### **2.3.4. Architectural Design Choices**

Overlapping with the "Knowledge-Guided Architecture" paradigm, this involves designing the ML model's architecture to intrinsically reflect known structures, processes, or constraints from the domain.1 This can include:

* Embedding logical rules or algebraic formulations directly into neural network layers.  
* Designing network components that mirror the steps of a known physical process.  
* Using Graph Neural Networks (GNNs) where the graph structure (nodes and edges) explicitly represents known physical interactions, dependencies, or relationships between components of a system.23 For instance, in modeling fluid flow through a network of pipes, the GNN's graph could represent the pipe network itself.  
* Designing recurrent cells in Recurrent Neural Networks (RNNs) in a way that respects conservation laws (e.g., energy conservation in time-series modeling of physical systems 23).

The infusion of knowledge into machine learning models for optimization occurs across a wide spectrum of granularity, reflecting the diverse nature of both the knowledge itself and the ML techniques employed. At one end, knowledge can provide high-level conceptual guidance. For example, the careful selection of input features based on domain expertise represents a broad-stroke application of knowledge, fundamentally shaping what the model is allowed to "see" and learn from.8 Slightly more direct, but still behavioral, is the imposition of constraints such as monotonicity (e.g., an increase in input X should always lead to an increase or no change in output Y) or range limits on predictions (e.g., a probability must be between 0 and 1).8 These guide the overall behavior of the model without dictating its internal mechanics precisely.

Moving towards finer-grained integration, modifying loss functions to include penalties for deviations from physical laws (as in PINNs 8) embeds knowledge directly into the learning objective that the model strives to optimize. This method directly influences the parameters learned by the model to ensure adherence to these laws. At the most granular level, knowledge can be woven into the very fabric of the model architecture. Designing specific neural network layers or modules to explicitly mimic known physical processes, or embedding mathematical equations or logical rules within the network structure, represents a deep and intricate form of knowledge infusion.7 This spectrum of integration methods provides KGML with remarkable flexibility. Not all domain knowledge needs to be, nor can it always be, hard-coded as a precise differential equation. In many optimization scenarios, softer guidance through judicious feature selection, the application of high-level constraints, or the use of architectures with appropriate inductive biases (like the Pointer Networks and Transformers mentioned in the Deakin-Coventry scholarship context 21 for problems such as the TSP) is more suitable or feasible. This adaptability allows KGML to be applied effectively across a wide range of problems where the nature and formalization of available knowledge vary significantly.

### **2.4. Knowledge Representation in KGML Systems**

For domain knowledge to be effectively integrated into KGML systems, it must first be translated into a formal representation that machine learning algorithms can process and leverage. The choice of knowledge representation is critical, as it directly impacts how well the ML model can understand and utilize the embedded knowledge.

One of the most prominent and increasingly popular methods for representing structured domain knowledge is through **Knowledge Graphs (KGs)**.7 KGs are graph-based data structures that represent factual information as a collection of entities (nodes) and the relationships (edges) between them. Typically, knowledge in a KG is stored as triples of the form (head entity, relation, tail entity), for example, (Aspirin, treats, Headache).27 KGs are powerful for capturing complex, interconnected information prevalent in many scientific and engineering domains, such as biology (gene-protein interaction networks, drug-disease relationships), materials science (material-property relationships), and engineering systems (component hierarchies and dependencies).

To make the symbolic information in KGs amenable to ML models, which primarily operate on numerical data, **Knowledge Representation Learning (KRL)** techniques are employed.7 KRL aims to learn low-dimensional vector representations (embeddings) for the entities and relations within a KG. These embeddings capture the semantic meaning and relational patterns present in the KG, allowing ML models to perform tasks like link prediction (inferring missing relationships), entity classification, or using these embeddings as features for downstream tasks. For example, some KGML approaches have used one-hot encoders or more sophisticated embedding techniques like those generated by GraphSAGE or PubMedBERT to represent entities and their attributes for input into neural networks.27

Beyond KGs and their embeddings, domain knowledge can also be represented in other forms, including:

* **Logical Rules and Expressions:** If-then rules, first-order logic statements, or other logical formalisms can capture declarative knowledge and constraints.  
* **Mathematical Equations:** Algebraic equations, ordinary differential equations (ODEs), and partial differential equations (PDEs) are common representations for scientific laws and principles, particularly in physics and engineering.7  
* **Constraints:** As discussed earlier, constraints like monotonicity, range limits, or conservation principles can be directly represented and enforced.  
* **Heuristics and Procedures:** Algorithmic descriptions or procedural knowledge can also be a form of knowledge, often derived from expert systems or established best practices.

The effectiveness of any KGML system is profoundly linked to the chosen method of knowledge representation. A symbiotic relationship exists: the representation must accurately capture the essence of the domain knowledge, and it must also be in a form that the chosen ML model can effectively integrate and learn from. An inadequate or poorly chosen representation can act as a bottleneck, preventing the ML model from fully leveraging the available domain knowledge, even if that knowledge is accurate and highly relevant to the optimization problem at hand. For instance, if complex physical laws are represented in an overly simplified manner, the KGML model might produce physically inconsistent results. Conversely, if knowledge is represented in an extremely complex symbolic form that is difficult to translate into numerical operations suitable for a standard neural network, its utility might be diminished. The development of KRL techniques 26 is a direct attempt to bridge this gap for graph-structured knowledge, by transforming symbolic KGs into dense vector embeddings that are more readily consumable by deep learning models. The success of frameworks like KGML-xDTD in drug discovery 28, for example, is heavily reliant on its ability to represent biomedical knowledge as a KG and then employ graph-based reinforcement learning to navigate and find meaningful paths (which are sequences of learned representations) within that KG. Therefore, ongoing advancements in knowledge representation—particularly those that make complex domain knowledge more "ML-friendly" and interpretable by algorithms—are intrinsically tied to the progress and broader applicability of KGML in solving challenging optimization problems.

## **3\. Understanding Optimization Problems in the Context of KGML**

KGML techniques are applied to a diverse range of optimization problems, each with unique characteristics and challenges. The suitability and specific implementation of KGML depend heavily on the nature of the optimization task, whether it involves discrete choices, continuous variables, stringent constraints, or multiple conflicting objectives.

### **3.1. Combinatorial Optimization**

Combinatorial optimization problems involve finding an optimal object or configuration from a finite, but often astronomically large, set of discrete possibilities.13 The variables in these problems typically take on discrete values (e.g., integers, selections from a set). A hallmark of many important combinatorial optimization problems is their NP-hard nature, meaning that the computational time required to find an exact optimal solution tends to grow exponentially with the problem size, making exhaustive search infeasible for practical instances.13

**Examples:**

* **Travelling Salesperson Problem (TSP):** A classic NP-hard problem focused on finding the shortest possible tour that visits a given set of cities exactly once and returns to the starting city.13 The TSP is a significant focus of the Deakin-Coventry cotutelle PhD program.20  
* **Knapsack Problem:** Given a set of items, each with a specific weight and value, the objective is to select a subset of items to include in a knapsack such that the total weight does not exceed a given capacity, and the total value is maximized.13  
* **Set Covering Problem:** Aims to find a minimum-cost subcollection of sets from a larger family of sets, such that the union of the chosen subcollection covers all elements of a universal set.14  
* **Graph-based Problems:** These include the Steiner Tree Problem (finding a minimum-cost tree connecting a subset of terminal vertices), Minimum Spanning Tree (finding a minimum-cost tree connecting all vertices), and Shortest Path problems.15  
* **Facility Location Problem:** Involves determining the optimal placement of facilities to serve a set of clients while minimizing costs related to facility establishment and client servicing.15

Challenges:  
The primary challenge in combinatorial optimization is the "combinatorial explosion" of the search space as the number of items, nodes, or variables increases.13 This makes brute-force enumeration impossible. Consequently, designing effective and generalizable heuristics or approximation algorithms that can find high-quality solutions in a reasonable amount of time is a major research focus.16  
Role of KGML:  
KGML offers promising avenues to address these challenges:

* **Learning Heuristics or Solution Policies:** KGML techniques, particularly those involving reinforcement learning or sequence-to-sequence models, can learn heuristics or policies for constructing solutions step-by-step.16 For instance, Large Language Models (LLMs) are being explored for their ability to understand problem descriptions and generate novel heuristic variations or even complete solution strategies.22  
* **Search Space Pruning:** ML models, guided by domain knowledge, can learn to identify and prune portions of the search space that are unlikely to contain optimal solutions, thereby reducing the computational burden for exact solvers or other heuristic methods.16  
* **Problem Representation:** KGML can aid in developing more effective representations of combinatorial problems that are amenable to ML techniques. For example, Graph Neural Networks (GNNs) are naturally suited for problems defined on graphs, allowing the model to leverage the inherent structure (a form of knowledge) of the problem.31  
* **Advanced Architectures:** The Deakin-Coventry scholarship's focus on Pointer Networks and Transformers for problems like TSP exemplifies the use of knowledge-guided architectures. Pointer Networks can directly output a sequence of input indices (e.g., city order), while Transformers can capture complex dependencies in solution construction.21

For NP-hard combinatorial optimization problems, where the quest for exact solutions often becomes computationally prohibitive for instances of practical scale, KGML is catalyzing a significant paradigm shift. Instead of solely focusing on the traditional design of explicit solvers or hand-crafted heuristics, the emphasis is increasingly moving towards developing machine learning systems that *learn how to solve* these complex problems. This often involves learning effective heuristics, solution construction policies, or search strategies. The inherent computational complexity of problems like the TSP 13 means that conventional approaches rely either on exact algorithms, which are limited by problem size, or on manually designed heuristics, which may lack generalizability or guarantees of optimality.16 KGML introduces a novel approach: employing ML, guided by intrinsic problem knowledge and structural properties, to automatically discover, refine, or even generate these crucial heuristics.16 Techniques such as reinforcement learning are being adapted to train intelligent agents that can make sequential decisions in the process of constructing a solution, for example, deciding the next city to visit in a TSP tour.34 Furthermore, the advent of LLMs has opened up new possibilities, with research exploring their capacity to generate innovative heuristic ideas, translate high-level strategies into executable code, or suggest improvements to existing algorithms.22 The Deakin-Coventry scholarship's specific interest in applying Pointer Networks and Transformers to the TSP 21 is a clear illustration of this trend. These advanced neural architectures are capable of learning to output complex sequences or structured representations that directly correspond to solutions, effectively learning the mapping from problem instance to solution. Therefore, in the realm of combinatorial optimization, KGML is not merely about finding a single optimal solution for a given instance; it is about creating adaptive, learning-based solvers that can develop an understanding of the underlying structure and solution patterns of an entire class of challenging problems.

### **3.2. Continuous Optimization**

Continuous optimization problems are characterized by decision variables that can take any real value within a specified continuous range. Consequently, the feasible set of solutions, denoted as M, is uncountably infinite.12 These problems are fundamental in many areas of science and engineering, often arising from the modeling of physical systems, parameter estimation, and resource management.

Types:  
Continuous optimization encompasses several important categories 12:

* **Unconstrained Optimization:** The feasible set M is the entire n-dimensional Euclidean space (Rn), meaning there are no explicit constraints on the variables.  
* **Convex Optimization:** Both the objective function and the feasible set are convex. A key property of convex optimization problems is that any local minimum is also a global minimum, which significantly simplifies the search for optimal solutions.  
* **Quadratic Programming (QP):** Involves optimizing a quadratic objective function subject to linear equality and/or inequality constraints.  
* **Cone Programming:** A generalization of linear programming where the non-negativity constraints are replaced by cone constraints. Important sub-classes include Second Order Cone Programming (SOCP) and Semidefinite Programming (SDP), which allow for modeling a wider range of convex problems.

Challenges:  
Solving continuous optimization problems can present several difficulties:

* **Non-Convexity:** For non-convex problems, traditional gradient-based algorithms can easily get trapped in local minima, failing to find the global optimum.  
* **Convergence Speed:** Even for convex problems, the speed of convergence of iterative algorithms can be slow, especially for large-scale or ill-conditioned problems.14  
* **High Dimensionality:** As the number of variables increases, the "curse of dimensionality" can make optimization significantly more challenging.  
* **Physical Realizability:** Ensuring that solutions adhere to underlying physical laws or constraints is crucial, especially when ML models are used to approximate solutions or system dynamics.2

Role of KGML:  
KGML techniques can play a vital role in addressing these challenges in continuous optimization:

* **Accelerating Convergence:** ML models can be trained to predict better update steps or search directions for traditional iterative algorithms, potentially accelerating their convergence.14  
* **Learning Optimization Models:** When an accurate analytical formulation of the objective function or constraints is difficult to derive, KGML can learn these models or methods directly from data, guided by partial domain knowledge.14  
* **Ensuring Physical Consistency:** Physics-Informed Neural Networks (PINNs) are a prime example of KGML in continuous optimization. By incorporating physical laws (often expressed as PDEs) into the loss function, PINNs guide the neural network to learn solutions that are not only data-consistent but also physically plausible.17 This is crucial for problems in fluid dynamics, heat transfer, and other physical sciences.  
* **Handling Expensive Black-Box Functions:** Bayesian Optimization, a KGML-aligned technique, is particularly effective for optimizing continuous objective functions that are expensive to evaluate or for which no analytical form is known. It builds a surrogate model of the function and intelligently selects evaluation points.10

In the domain of continuous optimization, particularly where solutions are expected to conform to underlying physical principles, KGML—and notably, methods like Physics-Informed Neural Networks (PINNs)—serves as a critical bridge. It connects the idealized, often mathematically precise, models of physical systems (such as those described by partial differential equations) with the complexities and imperfections of real-world data. Purely data-driven machine learning approaches, when applied to such problems, might identify solutions that fit the observed data points well but inadvertently violate these fundamental physical laws. This can lead to predictions or optimized parameters that are unrealistic, unreliable, or physically impossible.2 KGML techniques, especially PINNs, address this by embedding these physical laws—often in the form of PDEs—as soft constraints directly within the loss function of the learning algorithm.8 This architectural choice compels the ML model, typically a neural network, to learn solutions that achieve a dual objective: they must not only match the available empirical data but also satisfy the known physics of the system. This acts as a powerful form of regularization, guiding the learning process towards more meaningful and robust solutions. This guidance is particularly vital in scenarios where observational data is sparse, noisy, or provides only indirect measurements of the quantities of interest, as the incorporated physical knowledge provides a strong inductive bias, reducing the model's reliance on data alone and enhancing its ability to generalize correctly.36 Consequently, KGML significantly bolsters the reliability and trustworthiness of ML-derived solutions for continuous optimization problems that are rooted in the physical sciences.

### **3.3. Constrained Optimization**

Constrained optimization problems are those where solutions must not only optimize an objective function but also satisfy a specific set of limitations, conditions, or rules, known as constraints.11 The process involves clearly defining the decision variables (what can be changed), the objective function (what needs to be maximized or minimized), and the constraints that delineate the feasible region of solutions.

A key distinction exists between constrained optimization and machine learning: constrained optimization is primarily concerned with making optimal decisions given available information and a defined model of the world (including constraints), whereas machine learning is focused on making inferences and learning patterns from data.11 Traditional constrained optimization methods do not inherently learn from data in the way ML models do.11

Challenges:  
The primary challenges in constrained optimization include:

* **Complexity of Constraints:** Real-world problems often involve numerous constraints that can be highly complex, non-linear, or even dynamically changing. Formulating and handling these effectively can be difficult.  
* **Ensuring Feasibility:** A critical aspect is ensuring that any proposed solution is feasible, i.e., it satisfies all specified constraints. This can be particularly challenging when ML components are used to generate parts of the solution, as ML models do not inherently guarantee constraint satisfaction.14  
* **Interactions Between Objective and Constraints:** The interplay between optimizing the objective function and satisfying constraints can lead to complex trade-offs and difficult optimization landscapes.

Role of KGML:  
KGML can significantly enhance the ability to solve constrained optimization problems:

* **Learning to Satisfy Constraints:** ML models can be trained to learn solutions that inherently satisfy certain constraints, or constraints can be incorporated into the ML model's learning process. This can be achieved by designing model architectures that respect specific constraints (e.g., output layers that ensure range constraints) or by adding penalty terms to the loss function for constraint violations.8  
* **Knowledge-Guided Constraint Definition:** Domain knowledge is crucial for identifying and accurately defining the relevant constraints that an ML model should consider. KGML provides a framework for translating this domain knowledge into a form usable by the ML system.  
* **Specialized Techniques for Constrained Problems:** Bayesian optimization, for example, has extensions to handle Expensive Constrained Optimization Problems (ECOPs). These methods model the constraints alongside the objective function, often building separate surrogate models for each constraint and using specialized acquisition functions that consider both objective improvement and the probability of satisfying constraints.10  
* **LLMs in Constraint Handling:** LLMs are being explored for their potential to assist in formulating constrained optimization problems from natural language descriptions or to suggest appropriate strategies for handling complex constraints.

A pivotal role of KGML in the context of constrained optimization is its ability to facilitate the "mathematization" or formalization of complex real-world constraints, many of which are initially expressed as qualitative rules, expert heuristics, or operational guidelines. Real-world optimization scenarios are frequently laden with such constraints, stemming from physical limitations, regulatory requirements, business logic, or accumulated expert wisdom.11 Purely data-driven ML approaches may struggle to infer or consistently respect these constraints, particularly if the constraints are not explicitly and abundantly represented in the training data, or if they involve intricate non-linear relationships.14 KGML offers a structured way to explicitly incorporate this diverse constraint knowledge. This integration can take several forms: knowledge-guided feature engineering can make the implications of certain constraints more apparent to the model 8; architectural design choices can build models that inherently respect specific types of constraints, such as ensuring outputs fall within a predefined range 8; and, perhaps most commonly, loss functions can be augmented with penalty terms that discourage or forbid constraint violations.8 For instance, in optimizing a manufacturing schedule, expert knowledge about operational limits, machine capacities, or material flow restrictions 38 can be encoded as explicit constraints for an ML-based scheduling system. This process of translating often nuanced domain constraints into a language that ML models can understand and act upon makes the overall optimization problem more tractable for ML techniques. It significantly increases the likelihood of generating solutions that are not only mathematically optimal with respect to the objective function but also practically feasible and useful in the real-world context.

### **3.4. Multi-Objective Optimization (MOO)**

Multi-objective optimization (MOO) deals with problems where more than one objective function needs to be optimized simultaneously.18 A common characteristic of MOO problems is that the objectives are often conflicting; improving one objective may lead to the degradation of another. For example, in product design, minimizing cost and maximizing performance are typically conflicting goals.

The central concept in MOO is **Pareto optimality**. A solution is Pareto optimal if no other feasible solution can improve at least one objective without worsening at least one other objective. The set of all Pareto optimal solutions forms the **Pareto front** (or Pareto frontier), which represents the optimal trade-offs between the conflicting objectives.18 The goal in MOO is often not to find a single "best" solution, but rather to identify or approximate this Pareto front, allowing a decision-maker to choose a preferred solution based on their specific priorities.

Challenges:  
Key challenges in MOO include:

* **Computational Cost of Finding Pareto Front:** Identifying the complete Pareto front, especially in problems with many objectives or complex search spaces, can be computationally very expensive.  
* **Complex Objective Relationships:** Understanding and modeling the intricate, often non-linear, relationships and conflicts between multiple objectives is difficult.18  
* **Incorporating Decision-Maker Preferences:** Effectively guiding the search towards regions of the Pareto front that are most relevant to a decision-maker's preferences can be challenging.  
* **Visualization and Selection:** Presenting a high-dimensional Pareto front to a decision-maker and facilitating the selection of a final solution requires effective visualization and interaction techniques.

Role of KGML:  
KGML can play a significant role in addressing these MOO challenges:

* **Efficient Pareto Front Approximation:** ML models can be used to learn surrogate models for the multiple objective functions, enabling a more efficient exploration and approximation of the Pareto front.18  
* **Knowledge-Guided Search:** Domain knowledge can be incorporated to guide the search for Pareto optimal solutions. This knowledge might pertain to desirable trade-off regions, known relationships between objectives, or constraints on acceptable objective values. This can help focus the search on the most relevant parts of the Pareto front.  
* **Informed Scalarization:** Domain knowledge can inform the design of meaningful scalarization functions, which combine multiple objectives into a single objective, thereby transforming the MOO problem into a single-objective one, albeit with a specific preference structure.  
* **Bayesian MOO:** Bayesian optimization techniques have been extended to multi-objective settings (e.g., using acquisition functions like Expected Hypervolume Improvement (EHVI) or Predictive Entropy Search for Multi-objective optimization (PESMO)). These methods can leverage prior knowledge about the objectives.10  
* **Applications in Materials Design:** A prominent application area is materials science, where KGML helps in designing novel materials that must simultaneously satisfy multiple, often conflicting, property constraints (e.g., strength vs. ductility, or catalytic activity vs. stability).18

In the realm of multi-objective optimization, where the aim is to find solutions that represent the best possible compromises among several, often competing, criteria, KGML can transform the search for Pareto optimal solutions from a potentially undirected and exhaustive exploration into a more focused and informed process. This is achieved by integrating domain knowledge about desirable trade-offs, known relationships between objectives, or specific regions of interest within the multi-dimensional objective space. MOO typically does not yield a single optimal solution but rather a set of non-dominated solutions known as the Pareto front.19 Without any guiding information, exploring this entire front, especially in problems with many objectives, can be computationally prohibitive and may yield many solutions that are not practically relevant.18 Domain knowledge provides this crucial guidance. For instance, in the field of materials design, where a new material might need to exhibit both high strength and high ductility (often conflicting properties), knowledge about which specific trade-offs between these properties are acceptable or particularly desirable for a target application can significantly narrow down the search space.18 KGML facilitates the integration of such knowledge by influencing various aspects of the MOO process: it can guide the feature engineering or model selection when building predictive models for each of the multiple objectives 18; it can inform the design of scalarization functions that aggregate multiple objectives into a single one in a way that reflects decision-maker preferences or known priorities 18; or it can be used to impose constraints on the search space for MOO algorithms, focusing them on regions known to yield feasible or desirable property combinations. Furthermore, Bayesian multi-objective optimization methods 10 employ sophisticated acquisition functions that can be biased by prior knowledge to explore the most promising regions of the Pareto front more efficiently. By making the exploration of the solution space more targeted and relevant to the specific problem context and domain requirements, KGML significantly enhances the practicality and effectiveness of MOO.

### **3.5. Bayesian Optimization: A Data-Efficient Approach for Complex Objectives**

Bayesian Optimization (BO) is a powerful sequential strategy for the global optimization of functions that are "black-box" (i.e., their analytical form is unknown) and expensive to evaluate.10 It is particularly well-suited for scenarios where each function evaluation entails a costly experiment, simulation, or computation. BO iteratively builds a probabilistic surrogate model of the unknown objective function and uses an acquisition function to intelligently decide the next point to sample, aiming to find the global optimum with a minimal number of evaluations.

**Core Principles:**

* **Probabilistic Surrogate Model:** BO typically employs a Gaussian Process (GP) as its surrogate model. A GP is a non-parametric model that defines a probability distribution over functions. It is characterized by a mean function (representing the expected value of the objective function) and a covariance function or kernel (representing the similarity between function values at different input points). The GP is updated using Bayes' rule each time a new data point (an evaluation of the objective function) is obtained, refining its posterior distribution over the objective function.10  
* **Acquisition Function:** The acquisition function guides the search for the optimum by quantifying the utility of evaluating the objective function at any given point in the search space. It balances **exploration** (sampling in regions of high uncertainty, where the true function value is unknown) and **exploitation** (sampling in regions where the surrogate model predicts high objective values). Common acquisition functions include 10:  
  * Probability of Improvement (PI): Maximizes the probability of finding a value better than the current best.  
  * Expected Improvement (EI): Calculates the expected amount of improvement over the current best value.  
  * Upper Confidence Bound (UCB): Balances the predicted mean and uncertainty (variance) of the objective function.  
  * Knowledge Gradient (KG): Maximizes the expected incremental value of a measurement.  
  * Thompson Sampling (TS): Samples a function from the GP posterior and optimizes this sample.  
  * Entropy-based Methods (e.g., Predictive Entropy Search, Max-value Entropy Search): Aim to maximize the information gained about the location of the global optimum or its value.

Addressing Optimization Challenges with BO:  
BO has been adapted to tackle a wide range of complex optimization scenarios 10:

* **High-Dimensional Optimization:** Addressed through techniques like random embeddings (REMBO), variable selection using sensitivity analysis, or assuming additive structures in the objective function.  
* **Combinatorial and Discrete Spaces:** Handled by using latent representations (e.g., mapping discrete inputs to a continuous latent space via VAEs), employing inherently discrete surrogate models (e.g., random forests, Tree-structured Parzen Estimator \- TPE), or designing specialized kernels for discrete structures.  
* **Noisy and Robust Optimization:** Managed by using robust GP models that account for noise in evaluations (e.g., heteroscedastic GPs) or by designing acquisition functions robust to noise.  
* **Expensive Constrained Optimization (ECOPs):** BO can model constraints alongside the objective, often using methods like augmented Lagrangian relaxation or acquisition functions that incorporate the probability of feasibility.  
* **Advanced Scenarios:** BO has been extended for multi-objective optimization (e.g., ParEGO, EHVI), multi-task optimization (leveraging information from related tasks), multi-fidelity optimization (using cheap, low-fidelity evaluations to guide expensive, high-fidelity ones), transfer/meta-learning (accelerating optimization on new tasks using experience from previous tasks), and parallel/batch BO (evaluating multiple points simultaneously).

Role of Knowledge in Bayesian Optimization:  
BO is inherently a knowledge-guided approach. Prior beliefs or domain knowledge about the objective function (e.g., its smoothness, characteristic length scales, expected range) are incorporated into the GP prior through the choice of the kernel function and its hyperparameters. Domain knowledge can also guide the definition of the search space, the initialization of the optimization process, or even the selection of the acquisition function.  
Bayesian Optimization stands out as a prime exemplar of KGML principles in action, particularly for optimization problems where function evaluations are "expensive," meaning they are time-consuming, resource-intensive, or financially costly. The core philosophy of KGML involves the integration of existing knowledge with machine learning techniques to achieve enhanced performance, especially when data is limited.1 Bayesian Optimization inherently embodies this by formally incorporating prior knowledge—in the form of beliefs about the unknown objective function—into a sophisticated learning framework. This framework consists of the Gaussian Process (GP) surrogate model and the acquisition function, which together enable intelligent, data-efficient decision-making.10 The GP surrogate model commences with a "prior" distribution over possible functions. This prior represents initial beliefs or knowledge about the objective function's likely behavior, such as its smoothness, the scale over which it varies, or its general shape.10 Such knowledge is often encoded in the selection of the kernel function (covariance function) for the GP and the initial values of its hyperparameters. As each (costly) function evaluation provides a new data point, this prior distribution is updated to a "posterior" distribution using Bayes' rule. This iterative refinement of the model's understanding of the objective function, based on new evidence, is a direct application of learning from data while being guided and constrained by prior knowledge.10 Subsequently, the acquisition function utilizes this knowledge-infused probabilistic model (the GP posterior) to decide where to sample next. This decision carefully balances the exploration of uncertain regions of the search space (where new discoveries might be made) with the exploitation of regions already known to yield good function values.10 This strategic decision-making process is entirely guided by the current state of knowledge encapsulated in the surrogate model. Therefore, Bayesian Optimization is not merely a passive optimization algorithm; it is an active learning system that continuously uses, updates, and refines its internal knowledge representation (the GP) to solve complex optimization problems with maximal efficiency. This makes it a powerful and illustrative example of Knowledge-Guided Machine Learning in practice.

The following table summarizes the key characteristics of these optimization problem categories and highlights the relevance of KGML:

Table 2: Overview of Optimization Problem Categories and KGML Relevance

| Category | Defining Characteristics | Key Examples (Deakin-Coventry Focus) | Primary Challenges | How KGML Addresses Challenges/Key KGML Techniques |
| :---- | :---- | :---- | :---- | :---- |
| **Combinatorial Optimization** | Discrete variables, finite but vast search space, often NP-hard.13 | Travelling Salesperson Problem (TSP) 21, Knapsack Problem 13, Set Covering 14, Graph Problems (Steiner Tree, Min Spanning Tree).15 | Combinatorial explosion, designing effective heuristics.13 | Learning heuristics (RL, LLMs 22), search space pruning 16, knowledge-guided architectures (Pointer Nets, Transformers for TSP 21). |
| **Continuous Optimization** | Variables take real values, uncountably infinite feasible set.12 | Parameter tuning, system identification, resource allocation problems with continuous variables. | Non-convexity (local minima), convergence speed 14, high dimensionality, ensuring physical plausibility.2 | Accelerating convergence 14, learning optimization models 14, Physics-Informed Neural Networks (PINNs) for physical consistency 17, Bayesian Optimization for black-box functions.10 |
| **Constrained Optimization** | Solutions must satisfy specific limitations/conditions.11 | Resource allocation with budget limits, scheduling with deadlines, engineering design with material strength constraints. | Complexity of constraints, ensuring feasibility of ML-generated solutions.14 | Learning to satisfy constraints (via loss or architecture 8), knowledge-guided constraint definition, Bayesian Optimization for ECOPs.10 |
| **Multi-Objective Optimization (MOO)** | Optimizing multiple, often conflicting, objectives simultaneously; finding Pareto optimal solutions.18 | Design trade-offs (cost vs. performance 19), materials design with multiple property targets.18 | Identifying full Pareto front, handling objective conflicts 18, incorporating preferences. | Efficient Pareto front approximation with ML 18, knowledge-guided search for relevant trade-offs, Bayesian MOO.10 |
| **Bayesian Optimization (Method)** | Sequential optimization for expensive black-box functions using surrogate models (GPs) and acquisition functions.10 | Hyperparameter tuning of complex models, experimental design, materials discovery. | Data efficiency for costly evaluations, balancing exploration/exploitation. | Inherently KGML: GP prior incorporates knowledge, acquisition function guides search based on current knowledge state. Handles high-D, discrete, constrained, MOO scenarios with extensions.10 |

## **4\. KGML Techniques for Enhancing Optimization Processes**

Knowledge-Guided Machine Learning offers a diverse toolkit of techniques specifically aimed at enhancing various stages of the optimization process. These techniques range from automating the design of crucial algorithmic components like heuristics to making the solutions themselves more trustworthy and understandable.

### **4.1. Improving and Automating Heuristic Design**

Heuristics are problem-solving strategies that aim to find good, albeit not necessarily provably optimal, solutions to complex optimization problems, especially those that are NP-hard, within a reasonable timeframe. The traditional design of effective heuristics is a labor-intensive process, often relying heavily on deep domain expertise, intuition, and extensive trial-and-error.16 KGML is emerging as a powerful approach to automate or significantly assist in this design process, leading to potentially more effective and adaptable heuristics.

A particularly cutting-edge development in this area is the leveraging of **Large Language Models (LLMs)** for heuristic generation and improvement. LLMs, with their vast pre-trained knowledge encompassing natural language, programming code, and scientific literature, can be prompted to:

* **Propose innovative heuristic variations and implementation strategies for existing algorithms.** For example, an LLM can analyze the code of a complex metaheuristic like Construct, Merge, Solve & Adapt (CMSA) for combinatorial optimization problems, understand its components, and suggest novel ways to integrate existing parameters into its heuristic selection mechanisms or even propose entirely new selection criteria based on advanced concepts like entropy to promote solution diversity.22 The LLM can also suggest low-level code optimizations or more efficient data structures.22  
* **Develop heuristics for emerging or niche optimization problems** where there is limited prior art or readily available human expertise.32 For such problems, specialized frameworks are being developed:  
  * The **Contextual Evolution of Heuristics (CEoH)** framework extends the earlier Evolution of Heuristics (EoH) approach by explicitly incorporating detailed problem-specific descriptions into the prompts given to an LLM. This enhanced contextual information is crucial for guiding the LLM to generate relevant and effective heuristics for less-known problems, such as the Unit-Load Pre-marshalling Problem (UPMP). CEoH employs an evolutionary procedure where heuristics (represented by both code and natural language "thoughts") are iteratively generated, evaluated, and refined by the LLM.32  
  * The **Co-evolution of Algorithms and Language Model (CALM)** framework introduces a hybrid approach that combines verbal guidance (through sophisticated prompt engineering and evolutionary operators for prompts) with numerical guidance. The numerical guidance is achieved by fine-tuning the LLM itself via reinforcement learning, based on the quality of the heuristics it generates. This allows the LLM to co-evolve with the heuristic search process, progressively improving its ability to design high-performing heuristics.33

These LLM-based approaches signify a shift towards a more collaborative model of heuristic design. Rather than LLMs acting as simple code generators, they are becoming creative partners capable of understanding problem context, proposing novel algorithmic ideas, and iteratively refining these ideas based on performance feedback. This has the potential to democratize and accelerate the discovery of high-quality heuristics for a wide range of optimization problems. The iterative dialogue often involved in refining LLM-generated code, including correcting bugs or misinterpretations 22, further mirrors a collaborative design process between the human researcher and the AI. This evolution positions LLMs not just as tools but as active contributors to the intellectual process of algorithm design.

### **4.2. Search Space Pruning and Reduction**

Many optimization problems, particularly combinatorial ones, are characterized by extremely large search spaces. Exploring this entire space to find an optimal solution is often computationally infeasible. KGML offers techniques to intelligently prune or reduce this search space by identifying and eliminating regions that are unlikely to contain optimal solutions.16 This preprocessing step can make the problem more tractable for subsequent exact solvers or other heuristic methods.

The core idea is to use ML models, guided by domain knowledge and trained on problem instances, to learn which elements or components of a problem instance are less likely to be part of an optimal solution. This is often based on interpretable local features of these elements. For instance, in the classical maximum clique enumeration problem (finding the largest fully connected subgraph in a given graph), a KGML framework was proposed to learn the simpler task of pruning nodes that are not part of any maximum clique. This approach was shown to prune a very large fraction of the input graph (e.g., around 99% of nodes in sparse graphs) while still enabling the detection of almost all maximum cliques. Such aggressive pruning resulted in several-fold speedups for state-of-the-art exact maximum clique algorithms.16 The pruning process can also be multi-stage, where successively trained classifiers focus on progressively harder-to-prune elements.16

While LLMs are more commonly associated with heuristic generation, their capabilities in pattern recognition and policy learning could potentially extend to suggesting or learning search space pruning strategies. For example, research in pruning large vision-language models involves searching for optimal pruning policies to enhance efficiency 41; similar principles of learned pruning, guided by problem-specific knowledge, could be adapted for traditional optimization problems.

The application of KGML for search space pruning, especially when it employs interpretable machine learning models, serves as a vital bridge connecting the pattern-recognition strengths of ML with the rigorous optimality guarantees of exact optimization algorithms. Many critical optimization problems, particularly those in the combinatorial domain, grapple with enormous search spaces that render exact solutions computationally intractable for realistically sized instances.13 Exact solvers, while capable of guaranteeing optimality, often struggle with this scalability issue.16 KGML offers a pathway to mitigate this by learning to intelligently identify and remove elements or regions of the search space that are highly unlikely to contribute to an optimal solution.16 If this pruning process is both aggressive (significantly reducing the problem size) and accurate (seldom removing elements that are part of a true optimal solution), the simplified, reduced problem can then be tackled much more efficiently by an exact solver. The interpretability of the pruning model, a feature emphasized in some KGML frameworks 16, is crucial here. It not only builds trust in the pruning process but can also yield valuable insights into the problem's structure, which might, in turn, inform improvements in the exact solver itself or in the problem formulation. This creates a synergistic relationship: ML does not aim to replace the exact solver but rather to augment its capabilities by simplifying its task, making it applicable to a broader and more complex range of problem instances. This targeted intervention is often more effective than attempting to have the ML model learn the entire solution from scratch, especially for problems with hard constraints or complex optimality criteria.

### **4.3. Enhancing Model Interpretability in Optimization**

A significant drawback of many conventional ML models, especially deep neural networks, is their "black-box" nature, making it difficult to understand how they arrive at their predictions or decisions.1 In the context of optimization, where solutions often inform critical decisions in science, engineering, or business, this lack of interpretability can be a major barrier to trust and adoption. KGML directly addresses this challenge by aiming to make the decision-making process of ML models more transparent and comprehensible to human experts.1

KGML enhances interpretability in several ways:

* **Knowledge-Guided Architectures:** By designing ML model architectures where components or layers correspond to known physical processes, system components, or logical steps, the model's internal workings become more aligned with human understanding of the domain.1  
* **Knowledge-Guided Learning:** Incorporating domain knowledge, such as physical laws, into the loss function (e.g., physics-informed regularization) ensures that the model's outputs are scientifically consistent. Solutions that adhere to known principles are inherently more interpretable and trustworthy.1  
* **Interpretable Base Models:** When KGML is used to learn heuristics or pruning rules, employing interpretable base models (e.g., decision trees, linear models, rule-based systems) can provide direct insights into the problem structure and the factors driving optimal decisions.16  
* **Explicit Explanation Generation:** Some KGML frameworks are specifically designed to generate explanations for their predictions. For example, the KGML-xDTD framework for drug repurposing not only predicts potential drug-disease treatments but also provides explanations in the form of paths through a biomedical knowledge graph, representing plausible mechanisms of action.28

Understanding *why* an optimization model suggests a particular solution is crucial for debugging the model, validating its reasoning, gaining new scientific insights, and ensuring its safe and effective deployment in critical real-world applications.

The contribution of KGML to enhancing interpretability in optimization is twofold, addressing both the transparency of the model itself and the plausibility of the solutions it generates. Traditional ML models are frequently criticized as "black boxes," which obscures their internal decision-making logic and can erode confidence, particularly when these models are applied to high-stakes optimization tasks where the rationale behind a solution is as important as the solution itself.1 KGML endeavors to improve this by, for instance, designing model architectures where individual components or layers map to understandable domain-specific processes or concepts 1, or by utilizing inherently interpretable base models for specific sub-tasks within the optimization framework.16 These approaches work towards increasing *model transparency*.

However, for an optimization solution to be truly "interpretable" and acceptable to a domain expert, it must also make sense within the established context of their field. A solution that, while statistically derived from data, violates known physical laws or operational common sense is unlikely to be trusted or implemented, regardless of how well the model itself is understood.3 KGML addresses this critical aspect by ensuring the *solution's plausibility*. Techniques such as physics-informed loss functions 1 directly enforce scientific consistency, compelling the model to generate outputs that align with established principles. A compelling example of this is the KGML-xDTD framework, which not only predicts drug-disease relationships but also provides explicit, KG path-based explanations that link the prediction to known biological pathways, thereby explaining *why* a particular prediction is made.28 Thus, KGML's impact on interpretability is comprehensive: it aims to demystify the model's internal operations while simultaneously grounding its outputs in established domain knowledge. This dual approach satisfies the ML developer's need for debugging and validation and the domain expert's requirement for valid, understandable, and actionable solutions.

### **4.4. Physics-Informed Neural Networks (PINNs) and their Relation to KGML**

Physics-Informed Neural Networks (PINNs) have emerged as a prominent and highly effective class of KGML techniques, particularly for problems involving physical systems described by differential equations.3 PINNs are neural networks that are trained not only to fit observed data but also to satisfy given physical laws, typically expressed as partial differential equations (PDEs) or ordinary differential equations (ODEs).

Methodologies:  
The core idea behind PINNs is to embed the physical laws directly into the neural network's training process, usually by incorporating the residuals of the governing differential equations into the loss function.8 The total loss function for a PINN typically comprises:

1. A data fidelity term: Measures the mismatch between the NN's predictions and observed data points (if available).  
2. A physics-informed regularization term: Measures how well the NN's output (and its derivatives, computed via automatic differentiation) satisfies the governing PDEs/ODEs over a set of collocation points within the domain.  
3. Terms for boundary and initial conditions: Ensure the solution respects the specified conditions at the boundaries of the domain and at the initial time.

By minimizing this composite loss function, the NN learns a solution that is consistent with both the data and the underlying physics.

Applications:  
PINNs have found widespread application in solving a variety of problems, including 3:

* Solving forward problems (predicting system behavior given parameters and initial/boundary conditions).  
* Solving inverse problems (inferring unknown parameters or functions within PDEs from observed data).  
* Simulating complex physical phenomena in fluid dynamics, solid mechanics, heat transfer, material science, quantum mechanics, and geophysics.

Comparison with Broader KGML:  
PINNs are a specific, yet highly impactful, instantiation of the "Knowledge-Guided Learning" paradigm within the broader KGML framework.3 While KGML encompasses diverse methods for integrating various types of knowledge (scientific laws, heuristics, symbolic rules, KGs), PINNs primarily focus on knowledge explicitly represented as differential equations. They represent a powerful way to inject well-understood physical principles into data-driven models.  
Advantages and Challenges:  
PINNs offer several advantages, such as the ability to learn solutions from sparse and noisy data, handle complex geometries without the need for mesh generation (unlike traditional numerical solvers like FEM/FVM), and solve high-dimensional PDEs where traditional methods become intractable.17 However, they also face challenges, including difficulties in training for highly non-linear or stiff PDEs, the choice of appropriate neural network architectures and optimizers, managing the balance between different terms in the loss function, ensuring data quality, and the computational cost of training, especially for large-scale problems.17 Recent research also explores variations like Physics-Informed Kolmogorov-Arnold Networks (PIKANs) which show comparable accuracy.43  
While KGML serves as a broad conceptual umbrella for integrating diverse forms of knowledge into machine learning, Physics-Informed Neural Networks (PINNs) represent a particularly successful and specialized application of KGML. They are sharply focused on continuous systems that are governed by well-defined differential equations. The significant impact of PINNs stems from their provision of a direct, mathematically rigorous, and powerful mechanism to enforce these well-understood physical laws during the optimization process inherent in neural network training. KGML, in its entirety, aims to integrate a wide spectrum of knowledge—ranging from formal scientific laws and mathematical equations to expert heuristics and even common-sense world knowledge—through various techniques such as modifying loss functions, designing specific model architectures, or employing knowledge-rich pretraining data.1 PINNs, in contrast, specifically target "scientific knowledge" that is already formalized as partial differential equations (PDEs) or ordinary differential equations (ODEs).3 They primarily operate under the "knowledge-guided learning" paradigm by incorporating these governing equations directly into the loss function, typically as a regularization term that penalizes deviations from these laws.8 This direct embedding of fundamental physical principles makes PINNs exceptionally potent for modeling, simulating, and optimizing physical systems where such laws are clearly established, for instance, in fluid dynamics, heat transfer, and structural mechanics.17 While other KGML methodologies might grapple with more abstract, qualitative, or heuristic forms of knowledge, PINNs offer a robust and principled way to ensure that learned solutions strictly adhere to first principles in continuous physical domains. The rapid proliferation of research on PINNs and their successful application across numerous scientific and engineering disciplines 17 clearly underscore the immense value of this specific type of knowledge integration for a substantial class of optimization and simulation problems.

The following table summarizes these KGML techniques for enhancing optimization:

Table 3: KGML Techniques for Optimization Enhancement

| Technique | Description | Key KGML Method(s) Involved | Specific Examples/Frameworks | Expected Benefits for Optimization |
| :---- | :---- | :---- | :---- | :---- |
| **Improving/Automating Heuristic Design** | Using ML to learn, generate, or refine heuristics for complex optimization problems. | Knowledge-Guided Learning (e.g., RL for policy learning), Knowledge-Guided Architecture (e.g., LLM prompt design), LLM-based code/idea generation. | CEoH 32, CALM 33, LLM-assisted CMSA improvement 22, ALTHEA.16 | Faster discovery of high-quality heuristics, adaptation to new problems, reduced reliance on manual expert design. |
| **Search Space Pruning** | Intelligently reducing the size of the search space by eliminating unpromising regions. | Interpretable ML models, Knowledge-Guided Learning (learning pruning rules). | ML-based pruning for Maximum Clique.16 | Makes NP-hard problems more tractable for exact solvers, speeds up heuristic search, improves solution quality by focusing search. |
| **Enhancing Model Interpretability** | Making ML models' decision-making processes more transparent and understandable. | Knowledge-Guided Architecture (mapping components to known processes), Knowledge-Guided Learning (ensuring scientific consistency), Interpretable base models. | KGML-xDTD (KG path explanations) 28, interpretable pruning models.16 | Increased trust in solutions, easier debugging, validation of model reasoning, potential for new scientific insights from model behavior. |
| **Physics-Informed Neural Networks (PINNs)** | NNs trained to solve problems governed by PDEs/ODEs by incorporating physics into the loss function. | Knowledge-Guided Learning (physics-informed loss). | PINNs for fluid dynamics, material science, inverse problems.3 | Solves PDEs/ODEs with sparse data, handles complex geometries, provides physically consistent solutions for continuous optimization. |

## **5\. Frameworks and Applications of KGML in Optimization**

The principles and techniques of KGML are being applied across a diverse range of domains to tackle complex optimization problems. This section explores general frameworks for knowledge integration and then delves into specific application areas, highlighting how KGML is driving innovation and improved performance.

### **5.1. General Frameworks for Integrating Domain Knowledge**

Beyond specific techniques, there is a growing interest in developing overarching frameworks and methodologies that facilitate the systematic integration of domain knowledge with machine learning for optimization tasks. These frameworks aim to provide structured approaches for combining diverse knowledge sources with appropriate ML models and learning strategies.

Some conceptual frameworks focus on establishing a layered architecture for engineering AI ecosystems. Such frameworks might delineate distinct layers for problem identification, AI infrastructure (hardware, software, networking), data foundation (collection, preprocessing, annotation), AI model development, and finally, deployment and monitoring.47 Within such a structure, domain knowledge plays a crucial role at multiple stages, from defining the problem and identifying constraints (problem identification layer) to guiding data annotation (data foundation layer) and influencing model design and validation (AI model development layer).

A concrete example of a system-level integration framework is seen in network traffic engineering, where the Hecate AI/ML optimization tool is integrated with the PolKA source routing tool.48 Hecate leverages ML (e.g., supervised learning for predicting future bandwidth) to make optimized routing decisions, which are then implemented by PolKA, a path-aware source routing system. This framework uses APIs and message queues for communication between the ML optimization component and the network control component, enabling autonomous and adaptive routing based on predicted network conditions and defined objectives (e.g., minimizing latency or maximizing utilization).49 Such integrations showcase how distinct ML optimization modules can be coupled with domain-specific execution systems.

The development of these general frameworks reflects a maturation in the field of KGML. As the variety of knowledge types (e.g., scientific laws, expert heuristics, world knowledge 7) and the array of integration methods (e.g., modifying loss functions, tailoring architectures, knowledge-rich pretraining 1) continue to expand, there is an increasing need for more versatile and adaptable approaches. Early KGML applications might have been highly customized and monolithic, designed for a single specific problem. However, the trend is now shifting towards more modular and composable frameworks. This modularity is evident in systems like the Hecate-PolKA integration, where distinct AI/ML optimization components (Hecate) and domain-specific operational components (PolKA) interact through well-defined interfaces like APIs, indicating a separation of concerns.49 Similarly, conceptual blueprints for Engineering AI 47 often propose layered architectures—encompassing stages like problem identification, AI infrastructure provisioning, and data foundation management—which inherently suggest a structured, component-based methodology. Even within LLM-based KGML systems, such as CEoH 32 and CALM 33, one can observe distinct modules dedicated to LLM interaction, evolutionary search, and performance evaluation. This move towards modularity is significant because it allows for greater flexibility in adapting KGML solutions to new optimization problems. It facilitates the easier incorporation of different or evolving knowledge sources and enables the leveraging of best-of-breed ML algorithms for specific sub-tasks within the larger KGML pipeline. Ultimately, this promotes the scalability, reusability, and broader applicability of KGML in tackling diverse and complex optimization challenges.

### **5.2. Case Study: Deakin-Coventry PhD Program Focus**

The Deakin-Coventry cotutelle PhD program provides a specific and relevant case study of current research directions in KGML for optimization.20 The program explicitly targets optimization problems known for their high computational complexity, namely the **Travelling Salesperson Problem (TSP)**, **Convex Hull**, and **Delaunay Triangulation**.21 These problems are foundational in computer science and operations research and serve as challenging benchmarks for novel algorithmic approaches.

The ML models slated for exploration within this program are indicative of the sophisticated techniques being brought to bear on these problems:

* **Pointer Networks:** These are neural network architectures specifically designed for problems where the output consists of a sequence of pointers or indices to elements in the input sequence. This structure is particularly well-suited for tasks like the TSP, where the solution is an ordered sequence of cities (which are part\_of the input set).20 The use of Pointer Networks is a direct application of knowledge-guided architecture, as the network structure itself is tailored to the combinatorial nature of the output.  
* **Transformer-based models:** Transformers, with their powerful attention mechanisms, have revolutionized sequence modeling and have shown great promise in capturing complex dependencies and patterns in various types of data. In the context of these optimization problems, Transformers could be used to learn effective heuristics, predict solution structures, or model the relationships between input components (e.g., points for Convex Hull/Delaunay Triangulation, cities for TSP).20

KGML principles are central to this research. For instance, in tackling Convex Hull or Delaunay Triangulation, geometric knowledge (e.g., properties of convex polygons, criteria for Delaunay edges) could be incorporated to guide the learning process of the ML models, perhaps by informing the feature representation, the network architecture, or the loss function. For the TSP, known effective heuristics or structural properties of optimal tours could be used to bias the learning of Pointer Networks or Transformers, making the learning process more efficient and leading to higher-quality solutions.

The choice of Pointer Networks and Transformer-based models within the Deakin-Coventry PhD program for addressing complex combinatorial optimization problems like TSP, Convex Hull, and Delaunay Triangulation signals a strategic emphasis on leveraging advanced sequence learning and attention-based neural architectures. This approach, guided by KGML principles, aims to enable these models to learn intricate combinatorial structures and solution-generation policies directly from data or problem specifications. The target problems are fundamentally about discovering optimal arrangements or sequences 21: the TSP seeks an optimal sequence of city visits; the Convex Hull problem involves identifying an ordered subset of points that form the boundary of a convex shape; and Delaunay Triangulation requires constructing a specific graph structure based on geometric criteria. Pointer Networks 21 were conceived precisely for tasks where the output is a sequence of indices pointing to elements from the input set, making them a natural and architecturally informed choice for problems like the TSP. This represents a form of knowledge guidance embedded within the model's design. Transformers 21, renowned for their powerful self-attention mechanisms, excel at capturing long-range dependencies and complex interrelationships within sequential or structured data. They possess the capability to learn to generate sequences or identify relevant components crucial for constructing solutions to these geometric and combinatorial challenges. The application of KGML principles in this context would involve steering these powerful, yet often data-intensive, models with existing knowledge pertinent to these problems. This could include incorporating geometric properties, known effective heuristics, or constraints defining valid solutions, thereby enhancing the models' learning efficiency and the quality of the solutions they produce. This sophisticated strategy represents an endeavor to have the machine learning model learn the *process* of constructing good solutions for these computationally hard combinatorial problems, moving beyond simple classification or regression tasks towards generative modeling of optimal structures.

### **5.3. Applications in Logistics and Supply Chain Management**

KGML is finding impactful applications in logistics and supply chain management, where optimization is key to efficiency, cost reduction, and sustainability.

* **Route Optimization:** ML models such as Linear Regression, XGBoost, Support Vector Machines (SVM), and clustering algorithms like K-Means and DBSCAN are employed to optimize vehicle travel routes. The objectives typically include minimizing travel distance and time, which in turn helps reduce fuel consumption and carbon emissions.50 Domain knowledge about road networks, traffic patterns, vehicle capacities and constraints, delivery time windows, and geographical features can be integrated to guide these ML models, leading to more realistic and effective routing solutions.  
* **Demand Forecasting:** Accurate demand forecasting is crucial for efficient inventory management and logistics planning. ML models, including Linear Regression, Multilayer Perceptron (MLP) regressors, and XGBoost, are used to predict future delivery volumes and patterns.50 Knowledge about factors influencing demand, such as seasonality, promotional events, economic indicators, and market trends, can be incorporated as features or constraints to improve forecast accuracy.  
* **Sustainable Logistics:** There is a strong focus on using AI and ML for sustainable logistics, particularly in reducing environmental impact. This includes developing AI-powered strategies for fuel efficiency and minimizing carbon footprints.50 Knowledge of fuel consumption models for different vehicle types, emission factors associated with various fuels and transport modes, and the impact of driving behavior on efficiency can be embedded into optimization frameworks.  
* **LLMs for Enhanced Supply Chain Optimization:** The OptiGuide framework illustrates an innovative use of LLMs in supply chain optimization.51 This system allows users to query optimization outcomes in natural language and explore "what-if" scenarios. While the core optimization might still be handled by traditional combinatorial optimization solvers, the LLM acts as an intelligent interface, leveraging its understanding of language and reasoning capabilities to interpret results and facilitate decision-making. A key feature is that proprietary data does not need to be sent to the LLM, addressing privacy concerns.51 This approach uses the LLM's broad world knowledge and reasoning abilities to make complex optimization outputs more accessible and actionable for human operators.

The application of KGML in logistics and supply chain management is progressively moving beyond pure automation towards the development of sophisticated hybrid human-AI decision support systems. While AI, including advanced ML techniques, can effectively optimize routes, forecast demand, and manage inventory 50, the inherent complexity and dynamism of real-world logistics operations mean that human oversight, intervention, and nuanced judgment often remain indispensable, especially when dealing with unforeseen events or factors not fully captured by the models. The OptiGuide framework 51 is a prime example of this evolution. It explicitly aims to bridge the gap between the outputs of automated supply chain optimization algorithms and human comprehension by employing LLMs to interpret these outcomes and interactively answer "what-if" queries posed in natural language. This is a form of KGML where the LLM's extensive "world knowledge" and sophisticated reasoning capabilities are harnessed to make the implications of optimization decisions more transparent. The critical need for interpretability in logistics 1 is driven by the requirement for stakeholders to trust, understand, and confidently act upon AI-driven recommendations. Therefore, the prevailing trend is not merely to replace human planners with automated systems but to augment their capabilities with KGML tools. These tools are designed to provide optimized suggestions along with clear explanations and interactive exploration features, fostering a more collaborative and effective decision-making environment.

### **5.4. Applications in Manufacturing and Scheduling**

Manufacturing processes and production scheduling are complex optimization domains that benefit significantly from KGML approaches, especially in the context of smart manufacturing (Industry 4.0).

* **Smart Manufacturing Scheduling:** Research has demonstrated the use of hybrid ML models, combining Reinforcement Learning (RL) with Genetic Algorithms (GA), to optimize production scheduling in dynamic smart manufacturing environments.38 These models are trained to handle fluctuating demand, variable machine availability, and workforce constraints. By integrating knowledge about the manufacturing processes, machine capabilities, inter-task dependencies, and scheduling rules (e.g., into the RL's state/action/reward design or the GA's fitness function and genetic operators), these hybrid systems have achieved substantial improvements in production efficiency, resource utilization, and reductions in machine downtime and energy consumption compared to traditional heuristic methods.38  
* **Job-Shop Scheduling Problem (JSSP):** The JSSP is a notoriously difficult combinatorial optimization problem. Research, including work at Coventry University, explores using ML to understand the relationship between JSSP instance features and scheduling efficiency.52 This involves learning from the structural properties of problem instances to predict how well certain heuristics might perform or to identify characteristics of difficult instances. Furthermore, ML techniques like Particle Swarm Optimization (PSO) are used to design instance generators that can create JSSP instances tailored to specific features or to challenge particular heuristics, aiding in the development and robust testing of new scheduling algorithms.52

In the manufacturing sector, KGML is proving to be a crucial enabler for the development of adaptive and resilient scheduling systems. These systems are designed to respond effectively in real-time to the inherent dynamism and unpredictability of modern smart factory environments, which often include fluctuating customer demand, unexpected machine breakdowns, and variable material availability. This marks a significant advancement from traditional static scheduling approaches or purely heuristic methods that may struggle to cope with such volatility. Smart manufacturing environments are characterized by dynamic processes that necessitate real-time decision-making to maintain efficiency and responsiveness.38 Traditional scheduling techniques frequently fall short when faced with the scale and complexity of these dynamic demands.38 The development of hybrid models, such as one combining Reinforcement Learning (RL) and Genetic Algorithms (GA) 38, explicitly addresses this by training on diverse scenarios that include dynamic demand, machine availability variations, and workforce constraints. RL, in particular, is well-suited for learning optimal policies in environments that change over time. The integration of knowledge about the manufacturing system—such as machine capabilities, process flows, precedence constraints, and setup times—is implicitly or explicitly embedded into the KGML framework. This knowledge informs the definition of the state space, action space, and reward function for the RL component, or guides the design of the fitness function and genetic operators for the GA component. This knowledge infusion allows the scheduling system not merely to optimize for a single, static scenario, but to learn robust and flexible policies that perform effectively under a wide spectrum of operational conditions. The outcome is a significant enhancement in production efficiency, resource utilization, and the overall resilience of the manufacturing operation to disruptions.

### **5.5. Applications in Engineering Design and Resource Allocation**

KGML is increasingly applied to complex engineering design and resource allocation problems, where optimal solutions must often satisfy numerous technical, safety, and economic criteria.

* **Automotive Electrical/Electronic (E/E) Architectures:** In the design of modern automotive E/E architectures, Integer Linear Programming (ILP)-based multi-objective optimization approaches are used for resource allocation. This involves mapping software components to available hardware resources (ECUs) while adhering to stringent ISO 26262 safety standards.53 These standards dictate requirements for reliability, freedom from interference (FFI) between software components, Automotive Safety Integrity Level (ASIL) compatibility, and probabilistic metrics for random hardware failures (PMHF). The optimization objectives typically include minimizing development costs and minimizing the maximum execution times of critical functions. Knowledge of the ISO 26262 standard, hardware capabilities, software task requirements, and ASIL decomposition rules is deeply embedded into the ILP formulation.53  
* **Wireless Resource Allocation:** The LLM-RAO (Large Language Model for Resource Allocation Optimizer) framework demonstrates the use of LLMs for complex resource allocation in dynamic wireless environments, such as the IEEE 802.11ax uplink.54 LLM-RAO utilizes a prompt-based tuning strategy to flexibly convey evolving task descriptions, objectives (e.g., maximizing data rate or proportional fairness), and constraints (e.g., QoS requirements, queue status) to an off-the-shelf LLM. The LLM then generates and refines resource allocation solutions. This approach has shown superior performance and adaptability compared to conventional deep learning and analytical methods, leveraging the LLM's vast pre-trained knowledge and reasoning capabilities.55  
* **Additive Manufacturing (Laser Powder Bed Fusion \- L-PBF):** Research at Deakin University focuses on using data-driven ML to optimize process parameters in L-PBF for metallic parts.56 Techniques like Gradient Boosting combined with Particle Swarm Optimization (GB-PSO) are used to predict outcomes like relative density based on process parameters (e.g., laser power, scan speed). Domain knowledge about material properties, machine parameters, and their expected influence on the final part quality guides the feature selection and model building. Interpretability tools like SHAP (SHapley Additive exPlanations) are used to understand the significance of different parameters, further refining the knowledge-guided optimization process by confining the search to the most influential parameters.56  
* **High Entropy Alloy (HEA) Design:** ML-based frameworks are being developed to accelerate the discovery of new HEAs with desired properties.57 These frameworks aim to efficiently explore the vast compositional space of HEAs, guided by design objectives (e.g., specific mechanical or corrosion resistance properties) and constraints. The ML models are often trained using data from physics-based thermo-mechanical simulations as well as experimental results, effectively integrating theoretical knowledge with empirical data to guide the search for promising HEA candidates.57

Engineering design and resource allocation tasks frequently involve navigating immense and intricate design spaces, often characterized by a combinatorial explosion of possibilities. These explorations are further complicated by the need to satisfy numerous, often conflicting, constraints related to performance, safety, cost, and regulatory compliance. KGML provides a potent set of methodologies for systematically tackling these challenges by integrating formal, codified knowledge—such as industry standards, physical limits, and mathematical properties—with data-driven exploration and sophisticated optimization techniques. Modern engineering systems, exemplified by automotive electrical/electronic (E/E) architectures 53 or advanced wireless communication networks 54, present an enormous number of potential configurations and resource allocation choices. The design objectives in such systems are typically multi-faceted; for instance, an automotive system might need to minimize development cost AND maximize real-time performance AND rigorously ensure functional safety according to standards like ISO 26262\.53 This formal knowledge, embodied in standards or technical specifications (e.g., IEEE 802.11ax for wireless networks 55), furnishes hard constraints and essential guidelines for the optimization process. KGML approaches, such as the Integer Linear Programming (ILP)-based method developed for ASIL decomposition in automotive systems 53, directly encode these stringent standards into the mathematical formulation of the optimization problem. In a different vein, the LLM-RAO framework 55 leverages the natural language understanding and generation capabilities of LLMs through sophisticated prompt engineering to convey complex objectives and constraints to the model, which then uses its embedded knowledge to reason about and propose solutions. Similarly, in the field of materials design, such as optimizing laser powder bed fusion processes 56 or discovering novel high-entropy alloys 57, KGML helps to dramatically narrow down the search for optimal material compositions or manufacturing process parameters from a potentially vast parameter space. By effectively combining explicit domain knowledge with advanced search and learning algorithms, KGML is instrumental in making these complex engineering optimization tasks more tractable, ultimately leading to solutions that are not only high-performing but also compliant, safe, and economically viable.

### **5.6. Applications in Bioinformatics and Drug Discovery**

The fields of bioinformatics and drug discovery are rich with complex data and intricate biological knowledge, making them fertile ground for KGML applications, particularly in optimization tasks related to identifying therapeutic candidates and understanding their mechanisms.

* **KGML-xDTD Framework:** A notable example is the Knowledge Graph-based Machine Learning framework for explainably predicting Drugs Treating Diseases (KGML-xDTD).28 This framework addresses two key challenges: predicting new uses for existing drugs (drug repurposing) and explaining the biological mechanisms underlying these predictions.  
  * **Structure:** KGML-xDTD consists of two main modules:  
    1. A **drug repurposing prediction module** that combines GraphSAGE (a graph neural network for learning node embeddings from a biomedical knowledge graph) with a Random Forest classifier. This module predicts the probability that a given drug can treat a specific disease.  
    2. A **Mechanism of Action (MOA) prediction module** that employs an adversarial actor-critic reinforcement learning (RL) model. This RL agent navigates the biomedical knowledge graph to find plausible biological paths from drug nodes to disease nodes, representing potential MOAs.  
  * **Knowledge Integration:** A crucial aspect of the MOA prediction module is its use of "demonstration paths." These are biologically meaningful paths extracted from the knowledge graph using existing biological knowledge (e.g., known drug-target interactions from databases like DrugBank, information from PubMed publications). These demonstration paths serve as guidance for the RL agent, biasing its search towards more biologically relevant pathways.28  
  * **Performance and Explainability:** KGML-xDTD has demonstrated state-of-the-art performance in both drug repurposing prediction and the recapitulation of human-curated MOA paths. Critically, it offers explainable predictions by providing these KG path-based MOAs, which can help reduce "black-box" concerns and increase confidence in computational drug discovery.28

In the intricate domains of bioinformatics and drug discovery, KGML is achieving more than just incremental improvements in predictive accuracy for tasks like drug repurposing. It is critically enabling the generation of testable scientific hypotheses regarding the underlying biological mechanisms of action. This shift from pure prediction to prediction coupled with explanation directly addresses the pervasive "black-box" problem often associated with complex ML models and, in doing so, significantly accelerates scientific understanding and the translation of computational findings into tangible biomedical research. The process of computational drug repurposing, which seeks to identify new therapeutic applications for existing drugs, offers a faster and more cost-effective alternative to de novo drug discovery.28 However, a significant bottleneck in this process has been the difficulty in elucidating the precise mechanism of action (MOA) by which a repurposed drug might affect a new disease target. Understanding this MOA is crucial for building confidence in the prediction and for guiding subsequent clinical development.28 The KGML-xDTD framework 28 is specifically designed to tackle this challenge. It comprises two interconnected modules: one dedicated to predicting potential drug-disease treatments and another focused on explaining these predictions by identifying plausible MOAs represented as paths within a comprehensive biomedical knowledge graph. The MOA prediction module utilizes sophisticated graph-based reinforcement learning, which is crucially guided by "demonstration paths"—pathways derived from existing, curated biological knowledge and publications. This guidance ensures that the RL agent explores biologically relevant mechanistic routes. By providing these KG path-based explanations, the KGML-xDTD framework directly mitigates concerns about the opacity of purely data-driven predictions and substantially increases the confidence in these predictions. This evolution from solely predictive models to systems that offer both prediction and mechanistic explanation, made possible by KGML's capacity to integrate and reason over vast, structured biological knowledge bases (such as KGs), is vital for transforming computational insights into actionable hypotheses and accelerating the pace of drug discovery and development.

### **5.7. Other Emerging Applications**

The versatility of KGML is evident in its application to a growing number of other complex optimization and modeling domains:

* **Energy Systems:** KGML is being used to optimize various aspects of energy systems. This includes merging fundamental thermodynamic principles with ML for enhancing energy conversion efficiency, managing virtual machine consolidation in cloud computing for energy savings, improving electricity load forecasting for better grid management, and optimizing the deployment and operation of renewable energy sources and energy storage systems.58 For instance, physics-based models of component wear and failure are combined with ML to predict failures in wind turbines, allowing for optimized maintenance schedules and reduced downtime.59  
* **Environmental Sciences:** This is a rapidly expanding area for KGML. Applications include improving the prediction of water quality dynamics in lakes and rivers (e.g., modeling lake surface temperature, forecasting phytoplankton blooms, predicting phosphorus concentrations) by integrating ecological knowledge and physical principles (like energy conservation or hydrodynamic models) with data-driven approaches.4 The development of Foundation Models for Environmental Science also explicitly considers the incorporation of KGML principles to build more robust and generalizable models for a wide range of environmental challenges, from climate change impacts to ecosystem management.62

The fundamental principles underpinning Knowledge-Guided Machine Learning—namely, the strategic integration of existing scientific understanding and domain expertise with advanced data-driven methodologies—are proving to be a broadly applicable and exceptionally powerful paradigm. This approach is effectively addressing complex optimization and modeling challenges across a remarkably diverse array of scientific and engineering disciplines, extending far beyond a few niche applications. Many scientific domains, from energy systems engineering to environmental ecosystem management, grapple with the inherent complexities of the systems they study. These systems are often characterized by a multitude of interacting components, highly non-linear dynamics, and the pervasive issue of incomplete, noisy, or sparse observational data.58 Traditional modeling approaches in these fields often face a dichotomy: purely physics-based or mechanistic models can be computationally prohibitive for large-scale systems or may be incomplete due to gaps in fundamental understanding, while purely data-driven machine learning models, though flexible, may lack generalizability, produce physically implausible results, or offer little in the way of mechanistic insight.3 KGML offers a compelling "middle way" by synergistically leveraging the strengths of both paradigms. For example, in the optimization of energy systems, established thermodynamic principles are being combined with ML algorithms to enhance efficiency and predict demand.58 Similarly, in environmental science, ecological knowledge, hydrological principles, and atmospheric physics are used to guide and constrain ML models for tasks such as water quality prediction, climate modeling, and pollution forecasting.60 Across these varied applications, several recurring themes highlight the value of KGML: the use of known scientific principles to constrain ML models, thereby reducing the search space and improving data efficiency; the enhancement of model interpretability, making solutions more transparent and trustworthy; and the critical assurance of physical consistency in predictions and optimized solutions. The recent emergence and advocacy for "Foundation Models for Environmental Science" 62, which explicitly incorporate KGML principles, further underscore a significant trend towards building more general-purpose, knowledge-infused models designed for broad scientific discovery. This suggests that KGML is evolving from a collection of specialized techniques into a fundamental and unifying approach to scientific machine learning, applicable wherever substantial domain knowledge coexists with available data.

The following table summarizes these KGML application case studies in optimization:

Table 4: Summary of KGML Application Case Studies in Optimization

| Application Domain | Specific Problem Addressed | Key KGML/ML Models/Frameworks Used | Types of Knowledge Integrated | Key Outcomes/Improvements |
| :---- | :---- | :---- | :---- | :---- |
| **Deakin-Coventry Focus** | TSP, Convex Hull, Delaunay Triangulation 21 | Pointer Networks, Transformers 21 | Combinatorial problem structure, geometric properties (implicit/explicit). | Development of novel ML solutions for high-complexity optimization. |
| **Logistics & Supply Chain** | Route optimization, demand forecasting, sustainable logistics.50 LLM for interpreting optimization outcomes.51 | Linear Regression, XGBoost, SVM, K-Means, DBSCAN, LLMs (OptiGuide).50 | Traffic patterns, network constraints, demand factors, fuel/emission models, supply chain logic. | Reduced emissions/costs, improved efficiency, enhanced human-AI interaction for decision support. |
| **Manufacturing & Scheduling** | Dynamic production scheduling in smart factories.38 JSSP analysis.52 | Hybrid RL+GA 38, ML for JSSP feature correlation & instance generation.52 | Manufacturing process rules, machine capabilities, dynamic constraints, JSSP structural features. | Increased efficiency (39%), resource utilization (14% up), reduced downtime (34%).38 Better understanding of JSSP. |
| **Engineering Design & Resource Allocation** | Automotive E/E resource allocation (ISO 26262).53 Wireless resource allocation (IEEE 802.11ax).55 L-PBF process optimization.56 HEA design.57 | ILP 53, LLM-RAO 55, GB-PSO 56, ML with physics-based models.57 | Safety standards, hardware/software properties, wireless protocols, material science, physics models. | Optimized cost/latency under safety rules, adaptive wireless allocation, improved material properties, accelerated alloy discovery. |
| **Bioinformatics & Drug Discovery** | Drug repurposing prediction, MOA explanation.28 | KGML-xDTD (GraphSAGE, RF, RL on KGs).28 | Biomedical KGs, known drug-target interactions, publication data, biological pathways. | State-of-the-art prediction, generation of explainable, biologically relevant MOA paths. |
| **Energy Systems** | Thermodynamic optimization, load forecasting, renewable energy management.58 | ML with thermodynamic principles, predictive analytics, hybrid physics-ML models.58 | Thermodynamic laws, system dynamics, component failure models. | Reduced error in power prediction, improved energy/exergy efficiencies, enhanced system reliability, cost savings from optimized maintenance. |
| **Environmental Sciences** | Water quality modeling (temperature, phytoplankton, phosphorus).60 | KGML with ecological/physical principles, Foundation Models.60 | Ecological knowledge, physical laws (energy conservation, hydrodynamics), climate models. | Improved prediction accuracy, better generalization, scientifically consistent environmental models. |

## **6\. Challenges, Future Directions, and Recommendations**

While KGML has demonstrated considerable promise in advancing optimization problem-solving, its widespread adoption and continued development face several challenges. Addressing these challenges will be crucial for unlocking the full potential of KGML. Concurrently, exciting future research prospects are emerging, pointing towards even more powerful and versatile KGML systems.

### **6.1. Current Challenges in KGML for Optimization**

Despite its successes, the practical application of KGML for optimization is not without its hurdles:

* **Knowledge Formalization and Representation:** A persistent challenge is the translation of diverse forms of domain knowledge—especially qualitative expert knowledge or implicit world knowledge—into precise, structured formats that ML models can effectively utilize.7 While scientific laws expressed as equations are relatively straightforward to incorporate (e.g., in PINNs 17), representing heuristics, causal relationships, or common-sense reasoning in a way that meaningfully guides ML remains difficult.  
* **Scalability:** Applying KGML to very large-scale optimization problems can be computationally demanding. Processing extensive knowledge bases (like large KGs) or training highly complex hybrid ML architectures (e.g., deep neural networks coupled with intricate knowledge-based modules) can require significant computational resources and time.17  
* **Balancing Data and Knowledge (Data Scarcity vs. Knowledge Reliability):** A critical issue is how to appropriately balance the influence of empirical data (which may be scarce, noisy, or incomplete) and domain knowledge (which may itself be imperfect, incomplete, or not entirely applicable to a new context).17 Determining how to proceed when data appears to contradict established knowledge, or how to weigh their relative contributions, is a non-trivial problem.  
* **Interpretability of Complex KGML Models:** While a primary goal of KGML is to enhance interpretability, the KGML models themselves can become highly complex, particularly those involving deep neural networks, large knowledge graphs, or intricate integration mechanisms. Understanding the precise interplay between the learned data patterns and the integrated knowledge in such systems can still be challenging.9  
* **Hyperparameter Tuning and Model Selection:** The integration of knowledge introduces additional design choices and hyperparameters into the ML pipeline (e.g., how to weight knowledge-based penalties in a loss function, how to structure knowledge-guided architectures). Optimizing this expanded set of choices for the KGML system itself can be a complex task.17  
* **Generalization to Out-of-Distribution Scenarios:** While knowledge guidance is intended to improve generalization, ensuring that KGML models perform robustly when faced with scenarios significantly different from those encountered during training remains an ongoing research area. The integrated knowledge must be general enough to apply to these new situations.2

A significant and overarching challenge in the field of Knowledge-Guided Machine Learning for optimization lies in what can be termed the "meta-optimization" problem. This refers to the complex task of determining how to optimally select the most relevant types of knowledge, choose the most effective methods for representing this knowledge, and decide on the most suitable strategies for integrating it with appropriate machine learning architectures and learning algorithms for any given optimization task. This challenge is compounded by the reality that both the available domain knowledge and the empirical data may be imperfect, incomplete, or contain uncertainties. KGML involves a multitude of design choices: identifying which specific pieces of knowledge are most crucial (from scientific laws to expert heuristics 7), deciding on the best way to formalize and represent this knowledge (e.g., as equations, rules, or within knowledge graphs 26), selecting the paradigm for integration (via the loss function, model architecture, or pretraining 1), and choosing the underlying ML model. The difficulty in formally capturing less structured forms of knowledge, such as tacit expert insights or common-sense reasoning, adds another layer of complexity.7 Furthermore, empirical data, especially in scientific and engineering domains, can often be scarce, noisy, or expensive to acquire.17 The knowledge itself, while generally reliable, might be incomplete, subtly incorrect in a new context, or its assumptions might not fully hold. The interaction between the explicitly integrated knowledge component and the data-driven learning component of a KGML system can be intricate, potentially leading to difficulties in training, tuning, and ensuring a synergistic, rather than conflicting, relationship between the two.17 Simply adding more knowledge does not invariably lead to better performance; the integration must be carefully orchestrated. Therefore, a critical area for future research is the development of systematic principles, methodologies, or even automated (perhaps ML-driven) approaches to guide the construction and configuration of KGML systems. This would effectively involve optimizing the KGML design and training pipeline itself to maximize its efficacy for specific optimization challenges.

### **6.2. Future Research Prospects**

The field of KGML for optimization is dynamic, with several exciting future research directions poised to further enhance its capabilities and impact:

* **Automated Knowledge Discovery and Integration:** A key frontier is the development of ML methods that can automatically discover relevant domain knowledge from diverse sources, such as scientific literature, experimental data, or large unstructured text corpora, and then seamlessly integrate this discovered knowledge into optimization models.  
* **Hybrid AI Systems:** Future systems are likely to see a deeper integration of KGML with other AI paradigms, such as symbolic reasoning, automated planning, and causal inference. This could lead to more powerful and versatile AI systems capable of complex reasoning and decision-making in optimization contexts.  
* **Lifelong and Continual KGML:** For optimization problems in dynamic or evolving environments, there is a need for KGML systems that can engage in lifelong or continual learning. Such systems would be able to continuously update and adapt both their knowledge bases and their ML models as new data becomes available or as the underlying system characteristics change over time.10  
* **Robust Uncertainty Quantification (UQ) in KGML:** Developing more sophisticated and reliable methods for quantifying uncertainty in the predictions and decisions of KGML models is crucial. This UQ should ideally account for uncertainties stemming from both the data (e.g., noise, sparsity) and the integrated knowledge (e.g., incompleteness, imprecision).10  
* **Ethical and Fair KGML:** As KGML systems are deployed in applications with societal impact (e.g., resource allocation, scheduling in public services), it will be vital to ensure that they do not inherit or amplify biases present in the training data or the codified knowledge. Research into fairness-aware KGML is essential.10  
* **Democratization of KGML:** Creating user-friendly tools, platforms, and methodologies that make it easier for domain experts—who may not be ML specialists—to build, customize, and deploy KGML models for their specific optimization problems will be key to broadening the adoption of these techniques.  
* **Advanced LLM Integration:** The role of Large Language Models in KGML for optimization is expected to expand significantly. Beyond heuristic generation, LLMs could be used for more sophisticated tasks such as: automatically formulating optimization problems from natural language descriptions; performing complex reasoning about problem structures and constraints; facilitating interactive, natural-language-based co-creation of optimization solutions with human experts; and even orchestrating complex KGML workflows by selecting appropriate knowledge sources and ML components.22  
* **Explainable AI (XAI) for KGML:** Pushing the boundaries of XAI to provide deeper and more intuitive explanations for how KGML models arrive at their solutions, especially clarifying the interplay between data-driven patterns and integrated knowledge.

A dominant and transformative future trajectory in the field of Knowledge-Guided Machine Learning for optimization will undoubtedly be the increasingly sophisticated and multifaceted integration of Large Language Models (LLMs). This trend is poised to move LLMs from being tools utilized for specific sub-tasks, such as heuristic generation or code optimization, to becoming core, integral components for knowledge representation, complex reasoning, automated problem formulation, and even the orchestration of the entire KGML workflow itself. LLMs, by virtue of their extensive pretraining on diverse and massive datasets, inherently possess vast amounts of embedded "world knowledge" and "expert knowledge," albeit often in an implicit, distributed form.22 They have already demonstrated remarkable capabilities in understanding and generating human language, writing and interpreting program code, and even proposing novel algorithmic ideas or heuristic variations when appropriately prompted.22 Current applications showcase LLMs assisting in the design of heuristics through frameworks like CEoH 32 and CALM 33, improving existing optimization algorithms 22, and aiding in dynamic resource allocation as seen in LLM-RAO.55 Looking ahead, future research is likely to explore the potential of LLMs in even more integral roles within KGML systems. This includes leveraging LLMs for interpreting and formalizing domain knowledge from unstructured sources, such as scientific publications, technical manuals, or expert notes, and translating this into KG-usable formats like knowledge graphs or logical rules. LLMs could also significantly assist in the automatic formulation of complex optimization problems based on high-level, natural language descriptions provided by domain experts. Furthermore, their generative capabilities can be harnessed to provide interactive, natural language explanations for the decisions and solutions proposed by KGML models, enhancing transparency and trust. Ultimately, LLMs might even function as intelligent orchestrators for complex KGML pipelines, dynamically selecting the most appropriate knowledge sources, machine learning models, and integration strategies based on the problem at hand. This deeper and more versatile integration of LLMs promises to significantly lower the barrier to entry for applying KGML and could substantially broaden its impact across a diverse spectrum of challenging optimization problems.

### **6.3. Recommendations for Advancing the Field**

To accelerate progress and realize the full potential of KGML in optimization, several strategic actions are recommended:

* **Foster Interdisciplinary Collaboration:** Progress in KGML inherently requires close collaboration between ML researchers, optimization experts, and scientists or practitioners from various application domains. Initiatives that promote and fund such interdisciplinary teams are crucial.67  
* **Develop Standardized Benchmarks and Datasets:** The creation of well-curated, publicly available benchmark problems, datasets, and knowledge bases specifically designed for evaluating KGML approaches in optimization is essential. These benchmarks should cover diverse problem types and incorporate metrics for assessing not only solution quality and efficiency but also interpretability, robustness, and the effectiveness of knowledge integration.17  
* **Invest in Robust and Scalable Knowledge Representation:** Continued research into advanced knowledge representation techniques that can capture complex, uncertain, and dynamic domain knowledge in formats that are both rich in semantics and efficiently processable by ML algorithms is needed.  
* **Promote Open-Source Tools and Platforms:** The development and dissemination of open-source software libraries, tools, and platforms for building and experimenting with KGML models would lower the barrier to entry for researchers and practitioners, fostering innovation and reproducibility.  
* **Address Ethical Considerations and Bias:** Proactive research is needed to understand and mitigate potential ethical issues and biases that can arise from the knowledge sources used or the ways knowledge is integrated into KGML systems. This is particularly important for applications in sensitive domains.  
* **Support Education and Training:** Educational programs and training initiatives that bridge the gap between AI/ML, optimization theory, and specific scientific or engineering domains are vital for cultivating a new generation of researchers and practitioners skilled in developing and applying KGML solutions. The Deakin-Coventry cotutelle program serves as an example of such focused educational efforts.20

The development and widespread adoption of standardized benchmarks, datasets, and evaluation protocols specifically tailored for Knowledge-Guided Machine Learning in the context of optimization problems are crucial catalysts for fostering rigorous scientific comparison, ensuring reproducibility of results, and accelerating overall progress in this rapidly evolving field. KGML for optimization is characterized by a rich diversity of approaches, methodologies, and application domains, as evidenced throughout the existing literature. However, this same diversity can make it challenging to systematically compare the efficacy of different KGML methods. Current evaluations are often conducted on domain-specific problems, using varied datasets, disparate knowledge sources, and inconsistent evaluation metrics, which can obscure true advancements and make it difficult to identify universally effective techniques.65 Standardized benchmarks, akin to those that have propelled progress in core machine learning areas (e.g., ImageNet for computer vision or GLUE for natural language understanding), provide a common, objective ground for assessing the performance of new methods.60 For KGML in optimization, such benchmarks would need to be thoughtfully designed to evaluate not only the quality of the optimization solutions (e.g., objective function value, convergence speed) but also critical aspects like the model's interpretability, its robustness to imperfections or variations in the integrated knowledge, the efficiency of the knowledge integration process itself, and its ability to generalize. The call for standardization and benchmarking in related fields, such as Physics-Informed Machine Learning for building performance simulation 25, further underscores this pressing need. By establishing such comprehensive and widely accepted benchmarks, the research community can more effectively measure progress, identify the genuine state-of-the-art, highlight persistent challenges, and strategically guide future research efforts towards the most critical and impactful areas within KGML for optimization.

## **7\. Conclusion**

Knowledge-Guided Machine Learning represents a paradigm shift in the application of artificial intelligence to optimization problems. By moving beyond purely data-driven approaches and strategically integrating diverse forms of domain knowledge—ranging from fundamental scientific laws to expert heuristics—KGML offers a pathway to developing optimization solutions that are not only more effective but also more robust, interpretable, data-efficient, and scientifically consistent. This is particularly critical for complex optimization challenges encountered in science, engineering, and industry, where solutions must be both computationally optimal and practically viable within the constraints of the real world.

The core strength of KGML lies in its ability to synergize the pattern-recognition capabilities of machine learning with the established principles and insights from various domains. This report has detailed the foundational principles of KGML, including the primary paradigms of knowledge-guided learning, architecture, and pretraining, and has categorized the types of knowledge amenable to integration. It has explored the diverse landscape of optimization problems—combinatorial, continuous, constrained, and multi-objective—and elucidated how KGML techniques, including specialized methods like Bayesian Optimization and Physics-Informed Neural Networks, are tailored to address their unique challenges. Furthermore, the examination of LLM-driven heuristic design, intelligent search space pruning, and explicit mechanisms for enhancing model interpretability showcases the innovative ways KGML is enhancing the entire optimization process.

Applications across logistics, manufacturing, engineering design, resource allocation, bioinformatics, energy systems, and environmental sciences underscore the broad applicability and transformative potential of KGML. Initiatives like the Deakin-Coventry cotutelle PhD program highlight the active academic pursuit of these advanced methodologies for tackling notoriously difficult optimization problems.

Despite significant progress, challenges remain, primarily centered around the formalization and representation of complex knowledge, the scalability of KGML systems, the delicate balance between data and knowledge, ensuring true interpretability in sophisticated models, and the overarching "meta-optimization" of the KGML design process itself. Future research is poised to address these through automated knowledge discovery, deeper integration with symbolic AI, development of lifelong learning KGML systems, robust uncertainty quantification, and a concerted effort towards ethical and fair AI. The convergence of KGML with the rapidly advancing capabilities of Large Language Models promises to unlock new frontiers in intelligent optimization.

To fully realize this potential, continued interdisciplinary collaboration, the development of standardized benchmarks, investment in advanced knowledge representation techniques, and the promotion of open-source tools are paramount. By addressing these challenges and pursuing these future directions, Knowledge-Guided Machine Learning is set to revolutionize how complex optimization problems are understood and solved, paving the way for more intelligent, reliable, and insightful decision-making across a multitude of critical domains.

#### **Works cited**

1. Knowledge-guided machine learning model with soil moisture for corn yield prediction under drought conditions \- arXiv, accessed on May 20, 2025, [https://arxiv.org/html/2503.16328](https://arxiv.org/html/2503.16328)  
2. KGML-Bridge-AAAI-25 \- Google Sites, accessed on May 20, 2025, [https://sites.google.com/vt.edu/kgml-bridge-aaai-25/](https://sites.google.com/vt.edu/kgml-bridge-aaai-25/)  
3. Physics-informed machine learning for building performance simulation-A review of a nascent field \- arXiv, accessed on May 20, 2025, [https://www.arxiv.org/pdf/2504.00937](https://www.arxiv.org/pdf/2504.00937)  
4. \[2403.15989\] Knowledge-guided Machine Learning: Current Trends and Future Prospects \- arXiv, accessed on May 20, 2025, [https://arxiv.org/abs/2403.15989](https://arxiv.org/abs/2403.15989)  
5. KGML-ag: a modeling framework of knowledge-guided machine learning to simulate agroecosystems \- GMD, accessed on May 20, 2025, [https://gmd.copernicus.org/articles/15/2839/2022/gmd-15-2839-2022.pdf](https://gmd.copernicus.org/articles/15/2839/2022/gmd-15-2839-2022.pdf)  
6. Knowledge-guided machine learning for county-level corn yield prediction under drought, accessed on May 20, 2025, [https://arxiv.org/html/2503.16328v2](https://arxiv.org/html/2503.16328v2)  
7. Theory-guided Data Science models, accessed on May 20, 2025, [https://iris.polito.it/retrieve/ce14e388-9fd0-47b9-9fca-be20e842cbe0/conv\_sintesi.pdf](https://iris.polito.it/retrieve/ce14e388-9fd0-47b9-9fca-be20e842cbe0/conv_sintesi.pdf)  
8. 8 Domain Knowledge – Supervised Machine Learning for Science, accessed on May 20, 2025, [https://ml-science-book.com/domain.html](https://ml-science-book.com/domain.html)  
9. Interpretable Machine Learning in Physics: A Review \- arXiv, accessed on May 20, 2025, [https://arxiv.org/html/2503.23616v1](https://arxiv.org/html/2503.23616v1)  
10. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/pdf/2206.03301](https://arxiv.org/pdf/2206.03301)  
11. How Constrained Optimization helps a SaaS company \- Georgian, accessed on May 20, 2025, [https://georgian.io/constrained-optimization-how-to-do-more-with-less/](https://georgian.io/constrained-optimization-how-to-do-more-with-less/)  
12. kam.mff.cuni.cz, accessed on May 20, 2025, [https://kam.mff.cuni.cz/\~hladik/DSO/text\_dso\_en.pdf](https://kam.mff.cuni.cz/~hladik/DSO/text_dso_en.pdf)  
13. What is the combinatorial optimization problem? \- Annealing Cloud ..., accessed on May 20, 2025, [https://annealing-cloud.com/en/knowledge/1.html](https://annealing-cloud.com/en/knowledge/1.html)  
14. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/pdf/2405.15251](https://arxiv.org/pdf/2405.15251)  
15. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/pdf/2409.0075](https://arxiv.org/pdf/2409.0075)  
16. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/abs/2001.01230](https://arxiv.org/abs/2001.01230)  
17. Understanding Physics-Informed Neural Networks: Techniques ..., accessed on May 20, 2025, [https://www.mdpi.com/2673-2688/5/3/74](https://www.mdpi.com/2673-2688/5/3/74)  
18. Multi-objective optimization in machine learning assisted materials ..., accessed on May 20, 2025, [https://www.oaepublish.com/articles/jmi.2024.108](https://www.oaepublish.com/articles/jmi.2024.108)  
19. Multi-objective optimization \- Wikipedia, accessed on May 20, 2025, [https://en.wikipedia.org/wiki/Multi-objective\_optimization](https://en.wikipedia.org/wiki/Multi-objective_optimization)  
20. accessed on January 1, 1970, [https://www.deakin.edu.au/study/fees-and-scholarships/scholarships/find-a-scholarship/deakin-coventry-cotutelle-knowledge-guided-machine-learning-for-optimization-problems](https://www.deakin.edu.au/study/fees-and-scholarships/scholarships/find-a-scholarship/deakin-coventry-cotutelle-knowledge-guided-machine-learning-for-optimization-problems)  
21. 27 Fully Funded PhD Programs at Coventry University, England, accessed on May 20, 2025, [https://fellowshipbard.com/27-fully-funded-phd-programs-at-coventry-university/](https://fellowshipbard.com/27-fully-funded-phd-programs-at-coventry-university/)  
22. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/abs/2502.08298](https://arxiv.org/abs/2502.08298)  
23. Knowledge-guided Machine Learning: Current Trends and Future Prospects \- arXiv, accessed on May 20, 2025, [https://arxiv.org/html/2403.15989v1](https://arxiv.org/html/2403.15989v1)  
24. Knowledge Guided Machine Learning for Extracting, Preserving, and Adapting Physics-aware Features, accessed on May 20, 2025, [https://par.nsf.gov/servlets/purl/10503423](https://par.nsf.gov/servlets/purl/10503423)  
25. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/pdf/2504.00937](https://arxiv.org/pdf/2504.00937)  
26. \[1812.10901\] Knowledge Representation Learning: A Quantitative Review \- arXiv, accessed on May 20, 2025, [https://arxiv.org/abs/1812.10901](https://arxiv.org/abs/1812.10901)  
27. Relations Prediction for Knowledge Graph Completion using Large Language Models, accessed on May 20, 2025, [https://arxiv.org/html/2405.02738v1](https://arxiv.org/html/2405.02738v1)  
28. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/abs/2212.01384](https://arxiv.org/abs/2212.01384)  
29. KGML-xDTD: a knowledge graph–based machine learning framework for drug treatment prediction and mechanism description | GigaScience | Oxford Academic, accessed on May 20, 2025, [https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad057/7246583](https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad057/7246583)  
30. \[2409.00075\] A survey on combinatorial optimization \- arXiv, accessed on May 20, 2025, [https://arxiv.org/abs/2409.00075](https://arxiv.org/abs/2409.00075)  
31. Learning General Representations Across Graph Combinatorial Optimization Problems, accessed on May 20, 2025, [https://openreview.net/forum?id=elmTU101oS](https://openreview.net/forum?id=elmTU101oS)  
32. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/abs/2503.03350](https://arxiv.org/abs/2503.03350)  
33. CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design, accessed on May 20, 2025, [https://arxiv.org/html/2505.12285v1](https://arxiv.org/html/2505.12285v1)  
34. Neural combinatorial optimization with reinforcement learning in industrial engineering: a survey \- ResearchGate, accessed on May 20, 2025, [https://www.researchgate.net/publication/389008680\_Neural\_combinatorial\_optimization\_with\_reinforcement\_learning\_in\_industrial\_engineering\_a\_survey](https://www.researchgate.net/publication/389008680_Neural_combinatorial_optimization_with_reinforcement_learning_in_industrial_engineering_a_survey)  
35. Optimization Methods, Challenges, and Opportunities for Edge Inference: A Comprehensive Survey \- MDPI, accessed on May 20, 2025, [https://www.mdpi.com/2079-9292/14/7/1345](https://www.mdpi.com/2079-9292/14/7/1345)  
36. Machine Learning with Physics Knowledge for Prediction: A Survey \- arXiv, accessed on May 20, 2025, [https://arxiv.org/html/2408.09840v2](https://arxiv.org/html/2408.09840v2)  
37. What are easy examples from daily life of constrained optimization?, accessed on May 20, 2025, [https://matheducators.stackexchange.com/questions/1545/what-are-easy-examples-from-daily-life-of-constrained-optimization](https://matheducators.stackexchange.com/questions/1545/what-are-easy-examples-from-daily-life-of-constrained-optimization)  
38. Optimization of Production Scheduling in Smart ... \- ResearchGate, accessed on May 20, 2025, [https://www.researchgate.net/publication/388190203\_Optimization\_of\_Production\_Scheduling\_in\_Smart\_Manufacturing\_Environments\_Using\_Machine\_Learning\_Algorithms](https://www.researchgate.net/publication/388190203_Optimization_of_Production_Scheduling_in_Smart_Manufacturing_Environments_Using_Machine_Learning_Algorithms)  
39. \[2505.12285\] CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design \- arXiv, accessed on May 20, 2025, [https://arxiv.org/abs/2505.12285](https://arxiv.org/abs/2505.12285)  
40. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/pdf/2505.12285](https://arxiv.org/pdf/2505.12285)  
41. EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models \- arXiv, accessed on May 20, 2025, [https://arxiv.org/html/2503.15369v1](https://arxiv.org/html/2503.15369v1)  
42. Evaluating Interpretable Reinforcement Learning by Distilling Policies into Programs \- arXiv, accessed on May 20, 2025, [https://arxiv.org/html/2503.08322v1](https://arxiv.org/html/2503.08322v1)  
43. \[2501.16371\] Which Optimizer Works Best for Physics-Informed Neural Networks and Kolmogorov-Arnold Networks? \- arXiv, accessed on May 20, 2025, [https://arxiv.org/abs/2501.16371](https://arxiv.org/abs/2501.16371)  
44. From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning \- arXiv, accessed on May 20, 2025, [https://arxiv.org/abs/2410.13228](https://arxiv.org/abs/2410.13228)  
45. Physics-Informed Neural Networks in Polymers: A Review \- PMC, accessed on May 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC12030369/](https://pmc.ncbi.nlm.nih.gov/articles/PMC12030369/)  
46. Predicting transformer temperature field based on physics‐informed neural networks | High Voltage \- IET Digital Library, accessed on May 20, 2025, [https://digital-library.theiet.org/doi/full/10.1049/hve2.12435?doi=10.1049%2Fhve2.12435](https://digital-library.theiet.org/doi/full/10.1049/hve2.12435?doi=10.1049/hve2.12435)  
47. Engineering Artificial Intelligence: Framework, Challenges, and Future Direction \- arXiv, accessed on May 20, 2025, [https://arxiv.org/html/2504.02269v2](https://arxiv.org/html/2504.02269v2)  
48. Framework for Integrating Machine Learning Methods for Path-Aware Source Routing, accessed on May 20, 2025, [https://arxiv.org/html/2501.04624v1](https://arxiv.org/html/2501.04624v1)  
49. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/pdf/2501.04624](https://arxiv.org/pdf/2501.04624)  
50. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/abs/2503.14556](https://arxiv.org/abs/2503.14556)  
51. \[2307.03875\] Large Language Models for Supply Chain Optimization \- arXiv, accessed on May 20, 2025, [https://arxiv.org/abs/2307.03875](https://arxiv.org/abs/2307.03875)  
52. Correlation of Job-Shop Scheduling Problem Features with Scheduling Efficiency | Request PDF \- ResearchGate, accessed on May 20, 2025, [https://www.researchgate.net/publication/303917347\_Correlation\_of\_Job-Shop\_Scheduling\_Problem\_Features\_with\_Scheduling\_Efficiency](https://www.researchgate.net/publication/303917347_Correlation_of_Job-Shop_Scheduling_Problem_Features_with_Scheduling_Efficiency)  
53. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/abs/2505.07881](https://arxiv.org/abs/2505.07881)  
54. Adaptive Resource Allocation Optimization Using Large Language Models in Dynamic Wireless Environments \- arXiv, accessed on May 20, 2025, [https://arxiv.org/html/2502.02287v1](https://arxiv.org/html/2502.02287v1)  
55. arxiv.org, accessed on May 20, 2025, [https://arxiv.org/pdf/2502.02287](https://arxiv.org/pdf/2502.02287)  
56. A generalized machine learning framework for data-driven ..., accessed on May 20, 2025, [https://www.researchgate.net/publication/385554673\_A\_generalized\_machine\_learning\_framework\_for\_data-driven\_prediction\_of\_relative\_density\_in\_laser\_powder\_bed\_fusion\_parts](https://www.researchgate.net/publication/385554673_A_generalized_machine_learning_framework_for_data-driven_prediction_of_relative_density_in_laser_powder_bed_fusion_parts)  
57. PRELIMINARY TECHNICAL PROGRAM \- The Minerals, Metals & Materials Society, accessed on May 20, 2025, [https://www.tms.org/portal/downloads/meetings/2019/hea2019/HEA2019PreliminaryProgram.pdf](https://www.tms.org/portal/downloads/meetings/2019/hea2019/HEA2019PreliminaryProgram.pdf)  
58. THERMODYNAMICS AND ENERGY SYSTEMS: MACHINE LEARNING FOR ENERGY OPTIMIZATION \- IJNRD, accessed on May 20, 2025, [https://ijnrd.org/papers/IJNRD2410133.pdf](https://ijnrd.org/papers/IJNRD2410133.pdf)  
59. 5 AI Case Studies in Energy \- VKTR.com, accessed on May 20, 2025, [https://www.vktr.com/ai-disruption/5-ai-case-studies-in-energy/](https://www.vktr.com/ai-disruption/5-ai-case-studies-in-energy/)  
60. LakeBeD-US: a benchmark dataset for lake water quality time series and vertical profiles \- ESSD Copernicus, accessed on May 20, 2025, [https://essd.copernicus.org/preprints/essd-2025-27/essd-2025-27.pdf](https://essd.copernicus.org/preprints/essd-2025-27/essd-2025-27.pdf)  
61. CBCT-to-sCT with deep learning – image synthesis \- Zenodo, accessed on May 20, 2025, [https://zenodo.org/records/15348855/files/PINN\_sCT\_beyond\_ESTRO2025\_20250506.pdf?download=1](https://zenodo.org/records/15348855/files/PINN_sCT_beyond_ESTRO2025_20250506.pdf?download=1)  
62. Foundation Models for Environmental Science: A Survey of Emerging Frontiers, accessed on May 20, 2025, [https://www.researchgate.net/publication/390569971\_Foundation\_Models\_for\_Environmental\_Science\_A\_Survey\_of\_Emerging\_Frontiers/download](https://www.researchgate.net/publication/390569971_Foundation_Models_for_Environmental_Science_A_Survey_of_Emerging_Frontiers/download)  
63. arXiv:2503.03142v1 \[cs.LG\] 5 Mar 2025, accessed on May 20, 2025, [https://arxiv.org/pdf/2503.03142?](https://arxiv.org/pdf/2503.03142)  
64. Foundation Models for Environmental Science: A Survey of Emerging Frontiers \- arXiv, accessed on May 20, 2025, [https://arxiv.org/pdf/2504.04280](https://arxiv.org/pdf/2504.04280)  
65. Time series predictions in unmonitored sites: a survey of machine learning techniques in water resources | Environmental Data Science \- Cambridge University Press & Assessment, accessed on May 20, 2025, [https://www.cambridge.org/core/journals/environmental-data-science/article/time-series-predictions-in-unmonitored-sites-a-survey-of-machine-learning-techniques-in-water-resources/F9EBD3D6DAB06DEB8B01DE337722AD54](https://www.cambridge.org/core/journals/environmental-data-science/article/time-series-predictions-in-unmonitored-sites-a-survey-of-machine-learning-techniques-in-water-resources/F9EBD3D6DAB06DEB8B01DE337722AD54)  
66. A spectrum of physics-informed Gaussian processes for regression in engineering \- White Rose Research Online, accessed on May 20, 2025, [https://eprints.whiterose.ac.uk/id/eprint/211427/1/a-spectrum-of-physics-informed-gaussian-processes-for-regression-in-engineering.pdf](https://eprints.whiterose.ac.uk/id/eprint/211427/1/a-spectrum-of-physics-informed-gaussian-processes-for-regression-in-engineering.pdf)  
67. Report on NSF Sponsored Workshop on AI-Enabled Scientific Revolution March 8-9, 2023, Alexandria, VA \- University Digital Conservancy, accessed on May 20, 2025, [https://conservancy.umn.edu/bitstreams/e92a1eb9-ea16-42d9-8399-f49a2b773855/download](https://conservancy.umn.edu/bitstreams/e92a1eb9-ea16-42d9-8399-f49a2b773855/download)